{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notebook for generating initial statistics across the whole brain for the spontaneous events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from janelia_core.dataprocessing.dataset import ROIDataset\n",
    "from janelia_core.stats.regression import grouped_linear_regression_acm_stats\n",
    "from janelia_core.stats.regression import grouped_linear_regression_ols_estimator\n",
    "\n",
    "from keller_zlatic_vnc.data_processing import count_unique_subjs_per_transition\n",
    "from keller_zlatic_vnc.data_processing import generate_standard_id_for_full_annots\n",
    "from keller_zlatic_vnc.data_processing import generate_standard_id_for_volume\n",
    "from keller_zlatic_vnc.data_processing import get_basic_clean_annotations_from_full\n",
    "from keller_zlatic_vnc.data_processing import read_full_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parmaeters go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = dict()\n",
    "\n",
    "# Folders containing a4 and a9 annotation data\n",
    "ps['a4_annot_folder'] = r'\\\\dm11\\bishoplab\\projects\\keller_vnc\\data\\full_annotations\\behavior_csv_cl_A4'\n",
    "ps['a9_annot_folder'] = r'\\\\dm11\\bishoplab\\projects\\keller_vnc\\data\\full_annotations\\behavior_csv_cl_A9'\n",
    "ps['spont_only_annot_folder'] = r'\\\\dm11\\bishoplab\\projects\\keller_vnc\\data\\full_annotations\\spontaneous_only_annotations'\n",
    "\n",
    "# File containing locations to registered volumes\n",
    "ps['volume_loc_file'] = r'\\\\dm11\\bishoplab\\projects\\keller_vnc\\data\\experiment_data_locations.xlsx'\n",
    "\n",
    "# List subjects we do not want to include in the analysis\n",
    "ps['exclude_subjs'] = set(['CW_17-11-06-L2'])\n",
    "\n",
    "# Specify the threshold we use (in number of stacks) to determine when a quiet transition has occured\n",
    "ps['q_th'] = 21\n",
    "\n",
    "# Subfolder containing the dataset for each subject\n",
    "ps['dataset_folder'] = 'extracted'\n",
    "\n",
    "# Base folder where datasets are stored \n",
    "ps['dataset_base_folder'] =r'K:\\\\SV4'\n",
    "\n",
    "# Data to calculate Delta F/F for in each dataset\n",
    "ps['f_ts_str'] = 'f_12_60_60'\n",
    "ps['bl_ts_str'] = 'bl_12_60_60_long'\n",
    "\n",
    "# Parameters for calculating dff\n",
    "ps['background'] = 100\n",
    "ps['ep'] = 20\n",
    "\n",
    "# Min number of subjects we must observe a transition in for us to analyze it\n",
    "min_n_subjs = 10\n",
    "\n",
    "# Alpha value for thresholding p-values when calculating stats\n",
    "ps['alpha'] = .05\n",
    "\n",
    "# Specify where we save results\n",
    "ps['save_folder'] = r'\\\\dm11\\bishoplab\\projects\\keller_vnc\\results\\whole_brain_spont_stats'\n",
    "ps['save_name'] = 'spont_12_60_60_long_bl_co_21.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of all subjects we can analyze\n",
    "\n",
    "These are those we have registered volumes for and annotations and they are not in the excluded subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all annotation files and the subjects they correspond to\n",
    "annot_file_paths = (glob.glob(str(Path(ps['a4_annot_folder']) / '*.csv')) + \n",
    "                    glob.glob(str(Path(ps['a9_annot_folder']) / '*.csv')) +\n",
    "                    glob.glob(str(Path(ps['spont_only_annot_folder']) / '*.csv')))\n",
    "annot_file_names = [Path(p).name for p in annot_file_paths]\n",
    "annot_subjs = [generate_standard_id_for_full_annots(fn) for fn in annot_file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in location of all registered volumes\n",
    "def c_fcn(str):\n",
    "    return str.replace(\"'\", \"\")\n",
    "converters = {0:c_fcn, 1:c_fcn}\n",
    "\n",
    "volume_locs = pd.read_excel(ps['volume_loc_file'], header=1, usecols=[1, 2], converters=converters)\n",
    "volume_subjs = [generate_standard_id_for_volume(volume_locs.loc[i,'Main folder'], \n",
    "                                                       volume_locs.loc[i,'Subfolder'])  for i in volume_locs.index]\n",
    "volume_inds = [i for i in volume_locs.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update name of one of the volume subjects to match the annotations (this is only needed for one subject)\n",
    "m_ind = np.argwhere(np.asarray(volume_subjs) == 'CW_17-11-03-L6')[0][0]\n",
    "volume_subjs[m_ind] = 'CW_17-11-03-L6-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_subjs = set(volume_subjs).intersection(set(annot_subjs))\n",
    "analyze_subjs = analyze_subjs - set(ps['exclude_subjs'])\n",
    "analyze_subjs = list(np.sort(np.asarray(list(analyze_subjs))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each subject we analyze, determine where it's annotation and volume data is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_dict = dict()\n",
    "for s_id in analyze_subjs:\n",
    "    volume_i = np.argwhere(np.asarray(volume_subjs) == s_id)[0][0]\n",
    "    annot_i = np.argwhere(np.asarray(annot_subjs) == s_id)[0][0]\n",
    "    subject_dict[s_id] = {'volume_main_folder': volume_locs.loc[volume_inds[volume_i], 'Main folder'],\n",
    "                          'volume_sub_folder': volume_locs.loc[volume_inds[volume_i], 'Subfolder'],\n",
    "                          'annot_file': annot_file_paths[annot_i]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the annotation data for all subjects we analyze\n",
    "\n",
    "We also generate cleaned and supplemented annotations here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = []\n",
    "for s_id, d in subject_dict.items():\n",
    "    tbl = read_full_annotations(d['annot_file'])\n",
    "    tbl['subject_id'] = s_id\n",
    "    annotations.append(tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = [get_basic_clean_annotations_from_full(annot) for annot in annotations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.concat(annotations, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now threshold transitions to determine when events were preceeded or succeeded by quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.loc[(annotations['start'] - annotations['beh_before_end']) > ps['q_th'], 'beh_before'] = 'Q'\n",
    "annotations.loc[(annotations['beh_after_start'] - annotations['end']) > ps['q_th'], 'beh_after'] = 'Q'\n",
    "\n",
    "annotations.drop(['beh_before_start', 'beh_before_end', 'beh_after_start', 'beh_after_end'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now see how many subjects we have for each transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subjs_per_trans = count_unique_subjs_per_transition(annotations, before_str='beh_before', after_str='beh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "      <th>F</th>\n",
       "      <th>H</th>\n",
       "      <th>O</th>\n",
       "      <th>P</th>\n",
       "      <th>Q</th>\n",
       "      <th>TL</th>\n",
       "      <th>TR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>B</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>F</td>\n",
       "      <td>5.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>H</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>O</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>P</td>\n",
       "      <td>11.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Q</td>\n",
       "      <td>14.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TL</td>\n",
       "      <td>30.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TR</td>\n",
       "      <td>32.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       B     F     H     O     P    Q    TL    TR\n",
       "B   11.0   3.0  13.0   1.0   6.0  0.0  29.0  33.0\n",
       "F    5.0  52.0  36.0   4.0  46.0  0.0  45.0  40.0\n",
       "H   17.0  12.0  15.0   4.0  12.0  0.0  34.0  38.0\n",
       "O    1.0  23.0   4.0   5.0   7.0  0.0   8.0   5.0\n",
       "P   11.0  47.0  18.0  10.0  17.0  0.0  25.0  27.0\n",
       "Q   14.0  52.0  24.0  31.0  28.0  0.0  18.0  13.0\n",
       "TL  30.0  44.0  17.0   2.0  25.0  0.0  22.0  52.0\n",
       "TR  32.0  41.0  21.0   2.0  22.0  0.0  53.0  12.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_subjs_per_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we read in the $\\frac{\\Delta F}{F}$ data for all subjects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dff(f, b, background=ps['background'], ep=ps['ep']):\n",
    "    return (f-b)/(b-background+ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering neural data for subject CW_17-08-23-L1\n",
      "Gathering neural data for subject CW_17-08-23-L2\n",
      "Gathering neural data for subject CW_17-08-23-L4\n",
      "Gathering neural data for subject CW_17-08-24-L4\n",
      "Gathering neural data for subject CW_17-08-24-L5\n",
      "Gathering neural data for subject CW_17-08-26-L1\n",
      "Gathering neural data for subject CW_17-08-26-L2\n",
      "Gathering neural data for subject CW_17-08-26-L4\n",
      "Gathering neural data for subject CW_17-08-26-L5\n",
      "Gathering neural data for subject CW_17-08-26-L6\n",
      "Gathering neural data for subject CW_17-08-27-L1\n",
      "Gathering neural data for subject CW_17-08-27-L2\n",
      "Gathering neural data for subject CW_17-08-27-L4\n",
      "Gathering neural data for subject CW_17-08-27-L5\n",
      "Gathering neural data for subject CW_17-08-28-L1\n",
      "Gathering neural data for subject CW_17-08-28-L2\n",
      "Gathering neural data for subject CW_17-08-29-L2\n",
      "Gathering neural data for subject CW_17-08-31-L1\n",
      "Gathering neural data for subject CW_17-09-01-L1\n",
      "Gathering neural data for subject CW_17-09-01-L2\n",
      "Gathering neural data for subject CW_17-09-01-L3\n",
      "Gathering neural data for subject CW_17-11-02-L3\n",
      "Gathering neural data for subject CW_17-11-03-L1\n",
      "Gathering neural data for subject CW_17-11-03-L2\n",
      "Gathering neural data for subject CW_17-11-03-L3\n",
      "Gathering neural data for subject CW_17-11-03-L5\n",
      "Gathering neural data for subject CW_17-11-03-L6-2\n",
      "Gathering neural data for subject CW_17-11-03-L7\n",
      "Gathering neural data for subject CW_17-11-04-L1\n",
      "Gathering neural data for subject CW_17-11-04-L2\n",
      "Gathering neural data for subject CW_17-11-04-L3\n",
      "Gathering neural data for subject CW_17-11-04-L4\n",
      "Gathering neural data for subject CW_17-11-05-L6\n",
      "Gathering neural data for subject CW_17-11-05-L7\n",
      "Gathering neural data for subject CW_17-11-06-L1\n",
      "Gathering neural data for subject CW_17-11-06-L3\n",
      "Gathering neural data for subject CW_17-11-07-L3\n",
      "Gathering neural data for subject CW_17-11-07-L4\n",
      "Gathering neural data for subject CW_17-11-07-L5\n",
      "Gathering neural data for subject CW_17-11-08-L1\n",
      "Gathering neural data for subject CW_17-11-08-L2\n",
      "Gathering neural data for subject CW_17-11-08-L3\n",
      "Gathering neural data for subject CW_17-11-26-L1\n",
      "Gathering neural data for subject CW_17-11-26-L2\n",
      "Gathering neural data for subject CW_17-11-26-L3\n",
      "Gathering neural data for subject CW_17-11-26-L4\n",
      "Gathering neural data for subject CW_17-11-26-L5\n",
      "Gathering neural data for subject CW_17-11-27-L1\n",
      "Gathering neural data for subject CW_17-11-27-L2\n",
      "Gathering neural data for subject CW_17-11-27-L3\n",
      "Gathering neural data for subject CW_17-11-27-L4\n",
      "Gathering neural data for subject CW_17-11-27-L5\n",
      "Gathering neural data for subject CW_17-11-28-L2\n",
      "Gathering neural data for subject CW_17-11-28-L4\n",
      "Gathering neural data for subject CW_17-11-28-L6\n",
      "Gathering neural data for subject CW_17-11-29-L1\n",
      "Gathering neural data for subject CW_17-11-29-L2\n",
      "Gathering neural data for subject CW_17-11-29-L3\n",
      "Gathering neural data for subject CW_17-11-29-L4\n",
      "Gathering neural data for subject CW_17-11-29-L5\n",
      "Gathering neural data for subject CW_17-11-29-L6\n",
      "Gathering neural data for subject CW_17-11-30-L2\n",
      "Gathering neural data for subject CW_17-12-11-L3\n"
     ]
    }
   ],
   "source": [
    "extracted_dff = dict()\n",
    "for s_id in analyze_subjs:\n",
    "    print('Gathering neural data for subject ' + s_id)\n",
    "    \n",
    "    # Load the dataset for this subject\n",
    "    data_main_folder = subject_dict[s_id]['volume_main_folder']\n",
    "    data_sub_folder = subject_dict[s_id]['volume_sub_folder']\n",
    "    \n",
    "    dataset_path = (Path(ps['dataset_base_folder']) / data_main_folder / data_sub_folder / \n",
    "                        Path(ps['dataset_folder']) / '*.pkl')\n",
    "    dataset_file = glob.glob(str(dataset_path))[0]\n",
    "    \n",
    "    with open(dataset_file, 'rb') as f:\n",
    "            dataset = ROIDataset.from_dict(pickle.load(f))\n",
    "            \n",
    "    # Calculate dff\n",
    "    f=dataset.ts_data[ps['f_ts_str']]['vls'][:]\n",
    "    b=dataset.ts_data[ps['bl_ts_str']]['vls'][:]\n",
    "    dff = calc_dff(f=f, b=b)\n",
    "    \n",
    "    # Get the dff for each event\n",
    "    s_events = annotations[annotations['subject_id'] == s_id]\n",
    "    for index in s_events.index:\n",
    "        event_start = s_events['start'][index]\n",
    "        event_stop = s_events['end'][index] + 1 # +1 to account for inclusive indexing in table\n",
    "        mn_vls = np.mean(dff[event_start:event_stop, :], axis=0)\n",
    "        extracted_dff[index] = mn_vls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations['dff'] = pd.Series(extracted_dff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of transitions we observe in enough subjects to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_trans = [[(bb, ab) for ab in n_subjs_per_trans.loc[bb].index if n_subjs_per_trans[ab][bb] >= min_n_subjs] \n",
    "                for bb in n_subjs_per_trans.index]\n",
    "analyze_trans = list(itertools.chain(*analyze_trans))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Down-select events in annotations to only those with transitions that we will analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_codes = [b[0] + b[1] for b in analyze_trans]\n",
    "annot_trans_codes = [annotations['beh_before'][i] + annotations['beh'][i] for i in annotations.index]\n",
    "keep_annots = np.asarray([True if code in keep_codes else False for code in annot_trans_codes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_annotations = annotations[keep_annots]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate our regressors and group indicator variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_events = len(analyze_annotations)\n",
    "n_analyze_trans = len(analyze_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids = analyze_annotations['subject_id'].unique()\n",
    "g = np.zeros(n_events)\n",
    "for u_i, u_id in enumerate(unique_ids):\n",
    "    g[analyze_annotations['subject_id'] == u_id] = u_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros([n_events, n_analyze_trans])\n",
    "for row_i in range(n_events):\n",
    "    event_trans_code = analyze_annotations.iloc[row_i]['beh_before'] + analyze_annotations.iloc[row_i]['beh']\n",
    "    event_trans_col = np.argwhere(np.asarray(keep_codes) == event_trans_code)[0][0]\n",
    "    x[row_i, event_trans_col] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now actually calculate our statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = np.stack(analyze_annotations['dff'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for calculating stats\n",
    "\n",
    "def stats_f(x_i, y_i, g_i, alpha_i):\n",
    "    beta, acm, n_grps = grouped_linear_regression_ols_estimator(x=x_i, y=y_i, g=g_i)\n",
    "    stats = grouped_linear_regression_acm_stats(beta=beta, acm=acm, n_grps=n_grps, alpha=alpha_i)\n",
    "    stats['beta'] = beta\n",
    "    stats['acm'] = acm\n",
    "    stats['n_grps'] = n_grps\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rois = dff.shape[1]\n",
    "full_stats = [stats_f(x_i=x, y_i=dff[:, r_i], g_i=g, alpha_i=ps['alpha']) for r_i in range(n_rois)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now save our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = {'ps': ps, 'full_stats': full_stats, 'beh_trans': analyze_trans}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(ps['save_folder']) / ps['save_name']\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(rs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ps['save_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_vls = np.stack([s['non_zero_p'] for s in full_stats])\n",
    "beta = np.stack([s['beta'] for s in full_stats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beh_stats = {b[0] + '_' + b[1]: {'p_values': p_vls[:, b_i], 'beta': beta[:, b_i]}\n",
    "             for b_i, b in enumerate(analyze_trans)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 42)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_vls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
