{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notebook for generating initial statistics across the whole brain for the spontaneous events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from janelia_core.dataprocessing.dataset import ROIDataset\n",
    "from janelia_core.stats.regression import linear_regression_ols_estimator\n",
    "from janelia_core.stats.regression import grouped_linear_regression_acm_stats\n",
    "from janelia_core.stats.regression import grouped_linear_regression_ols_estimator\n",
    "\n",
    "from keller_zlatic_vnc.data_processing import count_unique_subjs_per_transition\n",
    "from keller_zlatic_vnc.data_processing import count_transitions\n",
    "from keller_zlatic_vnc.data_processing import generate_standard_id_for_full_annots\n",
    "from keller_zlatic_vnc.data_processing import generate_standard_id_for_volume\n",
    "from keller_zlatic_vnc.data_processing import get_basic_clean_annotations_from_full\n",
    "from keller_zlatic_vnc.data_processing import read_full_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = dict()\n",
    "\n",
    "# Folders containing a4 and a9 annotation data\n",
    "#ps['annot_folders'] = [r'\\\\dm11\\bishoplab\\projects\\keller_vnc\\data\\full_annotations\\behavior_csv_cl_A4',\n",
    "#                      r'\\\\dm11\\bishoplab\\projects\\keller_vnc\\data\\full_annotations\\behavior_csv_cl_A9',\n",
    "#                      r'\\\\dm11\\bishoplab\\projects\\keller_vnc\\data\\full_annotations\\spontaneous_only_annotations']\n",
    "\n",
    "ps['annot_folders'] = [r'\\\\dm11\\bishoplab\\projects\\keller_vnc\\data\\full_annotations\\em_volume_behavior_csv']\n",
    "\n",
    "# File containing locations to registered volumes\n",
    "#ps['volume_loc_file'] = r'\\\\dm11\\bishoplab\\projects\\keller_vnc\\data\\experiment_data_locations.xlsx'\n",
    "ps['volume_loc_file'] = r'\\\\dm11\\bishoplab\\projects\\keller_vnc\\data\\EM_volume_experiment_data_locations.xlsx'\n",
    "\n",
    "# List subjects we do not want to include in the analysis\n",
    "ps['exclude_subjs'] = set(['CW_17-11-06-L2'])\n",
    "\n",
    "# Specify the threshold we use (in number of stacks) to determine when a quiet transition has occured\n",
    "ps['q_th'] = 4\n",
    "\n",
    "# Subfolder containing the dataset for each subject\n",
    "ps['dataset_folder'] = 'extracted'\n",
    "\n",
    "# Base folder where datasets are stored \n",
    "ps['dataset_base_folder'] =r'K:\\\\SV4'\n",
    "\n",
    "# Data to calculate Delta F/F for in each dataset\n",
    "ps['f_ts_str'] = 'f_1_5_5'\n",
    "ps['bl_ts_str'] = 'bl_1_5_5_long'\n",
    "\n",
    "# Parameters for calculating dff\n",
    "ps['background'] = 100\n",
    "ps['ep'] = 20\n",
    "\n",
    "# Min number of subjects we must observe a transition in for us to analyze it\n",
    "ps['min_n_subjs'] = 1\n",
    "\n",
    "# Min number of events we must observe a transition in for us to analyze it\n",
    "ps['min_n_events'] = 5\n",
    "\n",
    "# Alpha value for thresholding p-values when calculating stats\n",
    "ps['alpha'] = .05\n",
    "\n",
    "# Specify the window we pull dff from\n",
    "ps['window_type'] = 'start_locked' #'whole_event' 'start_locked'\n",
    "\n",
    "# If we are using a window locked to event start or stop, we give the relative offset and window length here\n",
    "ps['window_offset'] = -18\n",
    "ps['window_length'] = 3\n",
    "\n",
    "# Specify if we only consider events where the extracted dff window is entirely contained within the event\n",
    "ps['enforce_contained_events'] = False\n",
    "\n",
    "# True if we want to pool preceeding behaviors\n",
    "ps['pool_preceeding_behaviors'] = True\n",
    "\n",
    "# True if we want to pool preceeding left and right turns into one category (only applies if pool_preceeding_behaviors is false)\n",
    "ps['pool_preceeding_turns'] = True\n",
    "\n",
    "# True if we want to pool succeeding left and right turns into one category\n",
    "ps['pool_succeeding_turns'] = False\n",
    "\n",
    "# The defintion we use for clean events\n",
    "ps['clean_event_def'] = 'disjoint' # 'decision' or 'disjoint'\n",
    "\n",
    "# List the types of behaviors we are interested in analyzing - this is for the behaviors we transition into. If None, we don't\n",
    "# filter events by behavior\n",
    "ps['behaviors'] = ['B', 'F', 'H', 'TR', 'TL']\n",
    " \n",
    "# Specify where we save results\n",
    "ps['save_folder'] = r'\\\\dm11\\bishoplab\\projects\\keller_vnc\\results\\single_subject_small_window_sweep'\n",
    "ps['save_name'] = 'beh_stats_neg_18_3_turns_broken_out.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of all subjects we can analyze\n",
    "\n",
    "These are those we have registered volumes for and annotations and they are not in the excluded subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all annotation files and the subjects they correspond to\n",
    "annot_file_paths = list(itertools.chain(*[glob.glob(str(Path(folder) / '*.csv')) for folder in ps['annot_folders']]))\n",
    "annot_file_names = [Path(p).name for p in annot_file_paths]\n",
    "annot_subjs = [generate_standard_id_for_full_annots(fn) for fn in annot_file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in location of all registered volumes\n",
    "def c_fcn(str):\n",
    "    return str.replace(\"'\", \"\")\n",
    "converters = {0:c_fcn, 1:c_fcn}\n",
    "\n",
    "volume_locs = pd.read_excel(ps['volume_loc_file'], header=1, usecols=[1, 2], converters=converters)\n",
    "volume_subjs = [generate_standard_id_for_volume(volume_locs.loc[i,'Main folder'], \n",
    "                                                       volume_locs.loc[i,'Subfolder'])  for i in volume_locs.index]\n",
    "volume_inds = [i for i in volume_locs.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update name of one of the volume subjects to match the annotations (this is only needed for one subject)\n",
    "m_ind = np.argwhere(np.asarray(volume_subjs) == 'CW_17-11-03-L6')\n",
    "if len(m_ind) > 0:\n",
    "    m_ind = m_ind[0][0]\n",
    "    volume_subjs[m_ind] = 'CW_17-11-03-L6-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_subjs = set(volume_subjs).intersection(set(annot_subjs))\n",
    "analyze_subjs = analyze_subjs - set(ps['exclude_subjs'])\n",
    "analyze_subjs = list(np.sort(np.asarray(list(analyze_subjs))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each subject we analyze, determine where it's annotation and volume data is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_dict = dict()\n",
    "for s_id in analyze_subjs:\n",
    "    volume_i = np.argwhere(np.asarray(volume_subjs) == s_id)[0][0]\n",
    "    annot_i = np.argwhere(np.asarray(annot_subjs) == s_id)[0][0]\n",
    "    subject_dict[s_id] = {'volume_main_folder': volume_locs.loc[volume_inds[volume_i], 'Main folder'],\n",
    "                          'volume_sub_folder': volume_locs.loc[volume_inds[volume_i], 'Subfolder'],\n",
    "                          'annot_file': annot_file_paths[annot_i]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the annotation data for all subjects we analyze\n",
    "\n",
    "We also generate cleaned and supplemented annotations here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = []\n",
    "for s_id, d in subject_dict.items():\n",
    "    tbl = read_full_annotations(d['annot_file'])\n",
    "    tbl['subject_id'] = s_id\n",
    "    annotations.append(tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = [get_basic_clean_annotations_from_full(annot, clean_def=ps['clean_event_def']) \n",
    "               for annot in annotations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.concat(annotations, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter events by the behavior transitioned into if we are suppose to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ps['behaviors'] is not None:\n",
    "    keep_inds = [i for i in annotations.index if annotations['beh'][i] in ps['behaviors']]\n",
    "    annotations = annotations.iloc[keep_inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now threshold transitions to determine when events were preceeded or succeeded by quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.loc[(annotations['start'] - annotations['beh_before_end']) > ps['q_th'], 'beh_before'] = 'Q'\n",
    "annotations.loc[(annotations['beh_after_start'] - annotations['end']) > ps['q_th'], 'beh_after'] = 'Q'\n",
    "\n",
    "annotations.drop(['beh_before_start', 'beh_before_end', 'beh_after_start', 'beh_after_end'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pool preceeding behaviors into one (G)rouped label if requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ps['pool_preceeding_behaviors']:\n",
    "    annotations['beh_before'] = 'G'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pool preceeding turns if requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ps['pool_preceeding_turns']:\n",
    "    turn_rows = (annotations['beh_before'] == 'TL') | (annotations['beh_before'] == 'TR')\n",
    "    annotations.loc[turn_rows, 'beh_before'] = 'TC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull succeeding turns if requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ps['pool_succeeding_turns']:\n",
    "    turn_rows = (annotations['beh'] == 'TL') | (annotations['beh'] == 'TR')\n",
    "    annotations.loc[turn_rows, 'beh'] = 'TC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we read in the $\\frac{\\Delta F}{F}$ data for all subjects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dff(f, b, background=ps['background'], ep=ps['ep']):\n",
    "    return (f-b)/(b-background+ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_dff(x, start, stop):\n",
    "\n",
    "    if ps['window_type'] == 'whole_event':\n",
    "        take_slice = slice(start, stop)\n",
    "        starts_within_event = True\n",
    "        stops_within_event = True\n",
    "    elif ps['window_type'] == 'start_locked':\n",
    "        start_offset = start + ps['window_offset']\n",
    "        stop_offset = start_offset + ps['window_length']\n",
    "        take_slice = slice(start_offset, stop_offset)\n",
    "        starts_within_event = ps['window_offset'] >= 0\n",
    "        stops_within_event = (stop >= stop_offset) and (start <= stop_offset)\n",
    "    else:\n",
    "        raise(ValueError('The window_type is not recogonized.'))\n",
    "    \n",
    "    if (take_slice.start < 0) or (take_slice.stop > x.shape[0]):\n",
    "        mn_vls = np.nan\n",
    "    else:\n",
    "        mn_vls = np.mean(x[take_slice, :], axis=0)\n",
    "    \n",
    "    return mn_vls, starts_within_event, stops_within_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering neural data for subject CW_18-02-15-L1\n"
     ]
    }
   ],
   "source": [
    "extracted_dff = dict()\n",
    "for s_id in analyze_subjs:\n",
    "    print('Gathering neural data for subject ' + s_id)\n",
    "    \n",
    "    # Load the dataset for this subject\n",
    "    data_main_folder = subject_dict[s_id]['volume_main_folder']\n",
    "    data_sub_folder = subject_dict[s_id]['volume_sub_folder']\n",
    "    \n",
    "    dataset_path = (Path(ps['dataset_base_folder']) / data_main_folder / data_sub_folder / \n",
    "                        Path(ps['dataset_folder']) / '*.pkl')\n",
    "    dataset_file = glob.glob(str(dataset_path))[0]\n",
    "    \n",
    "    with open(dataset_file, 'rb') as f:\n",
    "            dataset = ROIDataset.from_dict(pickle.load(f))\n",
    "            \n",
    "    # Calculate dff\n",
    "    f=dataset.ts_data[ps['f_ts_str']]['vls'][:]\n",
    "    b=dataset.ts_data[ps['bl_ts_str']]['vls'][:]\n",
    "    dff = calc_dff(f=f, b=b)\n",
    "    \n",
    "    # Get the dff for each event\n",
    "    s_events = annotations[annotations['subject_id'] == s_id]\n",
    "    for index in s_events.index:\n",
    "        event_start = s_events['start'][index]\n",
    "        event_stop = s_events['end'][index] + 1 # +1 to account for inclusive indexing in table\n",
    "        extracted_dff[index] = calc_mean_dff(dff, event_start, event_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove any events where the $\\Delta F /F$ window fell outside of the recorded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_keys = [k for k, vl in extracted_dff.items() if np.all(np.isnan(vl[0]))]\n",
    "for key in bad_keys:\n",
    "    del extracted_dff[key]\n",
    "    \n",
    "annotations.drop(bad_keys, axis='index', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put $\\Delta F/F$ into annotations table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations['dff'] = pd.Series({i:extracted_dff[i][0] for i in extracted_dff.keys()})\n",
    "annotations['starts_within_event'] = pd.Series({i:extracted_dff[i][1] for i in extracted_dff.keys()})\n",
    "annotations['stops_within_event'] = pd.Series({i:extracted_dff[i][2] for i in extracted_dff.keys()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enforce using only contained events if we need to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ps['enforce_contained_events']:\n",
    "    keep_events = (annotations['starts_within_event'] == True) & (annotations['stops_within_event'] == True)\n",
    "    annotations = annotations[keep_events]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now see how many subjects we have for each transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_subjs_per_trans = count_unique_subjs_per_transition(annotations, before_str='beh_before', after_str='beh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>TL</th>\n",
       "      <th>TR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>B</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>G</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>H</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      B    F    G    H   TL   TR\n",
       "B   0.0  0.0  0.0  0.0  0.0  0.0\n",
       "F   0.0  0.0  0.0  0.0  0.0  0.0\n",
       "G   1.0  1.0  0.0  1.0  1.0  1.0\n",
       "H   0.0  0.0  0.0  0.0  0.0  0.0\n",
       "TL  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "TR  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_subjs_per_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trans = count_transitions(annotations, before_str='beh_before', after_str='beh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>TL</th>\n",
       "      <th>TR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>B</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>G</td>\n",
       "      <td>54.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>H</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       B      F    G     H    TL    TR\n",
       "B    0.0    0.0  0.0   0.0   0.0   0.0\n",
       "F    0.0    0.0  0.0   0.0   0.0   0.0\n",
       "G   54.0  139.0  0.0  16.0  33.0  38.0\n",
       "H    0.0    0.0  0.0   0.0   0.0   0.0\n",
       "TL   0.0    0.0  0.0   0.0   0.0   0.0\n",
       "TR   0.0    0.0  0.0   0.0   0.0   0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of transitions we observe in enough subjects to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_trans = [[(bb, ab) for ab in n_subjs_per_trans.loc[bb].index \n",
    "                  if (n_subjs_per_trans[ab][bb] >= ps['min_n_subjs'] and n_trans[ab][bb] > ps['min_n_events'])] \n",
    "                for bb in n_subjs_per_trans.index]\n",
    "analyze_trans = list(itertools.chain(*analyze_trans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('G', 'B'), ('G', 'F'), ('G', 'H'), ('G', 'TL'), ('G', 'TR')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Down-select events in annotations to only those with transitions that we will analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_codes = [b[0] + b[1] for b in analyze_trans]\n",
    "annot_trans_codes = [annotations['beh_before'][i] + annotations['beh'][i] for i in annotations.index]\n",
    "keep_annots = np.asarray([True if code in keep_codes else False for code in annot_trans_codes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_annotations = annotations[keep_annots]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate our regressors and group indicator variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_events = len(analyze_annotations)\n",
    "n_analyze_trans = len(analyze_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids = analyze_annotations['subject_id'].unique()\n",
    "g = np.zeros(n_events)\n",
    "for u_i, u_id in enumerate(unique_ids):\n",
    "    g[analyze_annotations['subject_id'] == u_id] = u_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros([n_events, n_analyze_trans])\n",
    "for row_i in range(n_events):\n",
    "    event_trans_code = analyze_annotations.iloc[row_i]['beh_before'] + analyze_annotations.iloc[row_i]['beh']\n",
    "    event_trans_col = np.argwhere(np.asarray(keep_codes) == event_trans_code)[0][0]\n",
    "    x[row_i, event_trans_col] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now actually calculate our statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = np.stack(analyze_annotations['dff'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing stats for only one subject.\n"
     ]
    }
   ],
   "source": [
    "n_analyze_subjs = len(analyze_subjs)\n",
    "if n_analyze_subjs > 1:\n",
    "    print('Performing stats for multiple subjects.')\n",
    "    def stats_f(x_i, y_i, g_i, alpha_i):\n",
    "        beta, acm, n_grps = grouped_linear_regression_ols_estimator(x=x_i, y=y_i, g=g_i)\n",
    "        stats = grouped_linear_regression_acm_stats(beta=beta, acm=acm, n_grps=n_grps, alpha=alpha_i)\n",
    "        stats['beta'] = beta\n",
    "        stats['acm'] = acm\n",
    "        stats['n_grps'] = n_grps\n",
    "        return stats\n",
    "else:\n",
    "    print('Performing stats for only one subject.')\n",
    "    def stats_f(x_i, y_i, g_i, alpha_i):\n",
    "        n_grps = x_i.shape[0]\n",
    "        beta, acm  = linear_regression_ols_estimator(x=x_i, y=y_i)\n",
    "        stats = grouped_linear_regression_acm_stats(beta=beta, acm=acm, n_grps=n_grps, alpha=alpha_i)\n",
    "        stats['beta'] = beta\n",
    "        stats['acm'] = acm\n",
    "        stats['n_grps'] = n_grps\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bishopw\\documents\\research\\projects\\janelia_core\\janelia_core\\stats\\regression.py:474: RuntimeWarning: invalid value encountered in true_divide\n",
      "  non_zero_p = 2*scipy.stats.t(df=(n_grps-1)).cdf(-1*np.abs(beta/std_ers))\n",
      "C:\\Users\\bishopw\\AppData\\Local\\Continuum\\anaconda3\\envs\\keller_zlatic_vnc\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in greater\n",
      "  return (a < x) & (x < b)\n",
      "C:\\Users\\bishopw\\AppData\\Local\\Continuum\\anaconda3\\envs\\keller_zlatic_vnc\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:901: RuntimeWarning: invalid value encountered in less\n",
      "  return (a < x) & (x < b)\n",
      "C:\\Users\\bishopw\\AppData\\Local\\Continuum\\anaconda3\\envs\\keller_zlatic_vnc\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py:1807: RuntimeWarning: invalid value encountered in greater_equal\n",
      "  cond2 = (x >= _b) & cond0\n",
      "c:\\users\\bishopw\\documents\\research\\projects\\janelia_core\\janelia_core\\stats\\regression.py:474: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  non_zero_p = 2*scipy.stats.t(df=(n_grps-1)).cdf(-1*np.abs(beta/std_ers))\n"
     ]
    }
   ],
   "source": [
    "n_rois = dff.shape[1]\n",
    "full_stats = [stats_f(x_i=x, y_i=dff[:, r_i], g_i=g, alpha_i=ps['alpha']) for r_i in range(n_rois)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now save our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = {'ps': ps, 'full_stats': full_stats, 'beh_trans': analyze_trans}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(ps['save_folder']) / ps['save_name']\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(rs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('G', 'B'), ('G', 'F'), ('G', 'H'), ('G', 'TL'), ('G', 'TR')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('//dm11/bishoplab/projects/keller_vnc/results/single_subject_small_window_sweep/beh_stats_neg_18_3_turns_broken_out.pkl')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\\\\\dm11\\\\bishoplab\\\\projects\\\\keller_vnc\\\\results\\\\single_subject_small_window_sweep'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps['save_folder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beh_stats_neg_18_3_turns_broken_out.pkl'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps['save_name']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
