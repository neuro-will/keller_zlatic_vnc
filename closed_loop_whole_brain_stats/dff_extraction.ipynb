{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to extract and save $\\Delta F / F$ for whole making whole brain maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "#import re\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import janelia_core\n",
    "from janelia_core.dataprocessing.dataset import ROIDataset\n",
    "#from janelia_core.utils.data_saving import append_ts\n",
    "\n",
    "#from keller_zlatic_vnc.data_processing import extract_transitions\n",
    "from keller_zlatic_vnc.data_processing import calc_dff\n",
    "from keller_zlatic_vnc.data_processing import match_annotation_subject_to_volume_subject\n",
    "from keller_zlatic_vnc.data_processing import read_raw_transitions_from_excel\n",
    "#from keller_zlatic_vnc.data_processing import recode_beh\n",
    "from keller_zlatic_vnc.data_processing import down_select_events\n",
    "from keller_zlatic_vnc.data_processing import find_before_and_after_events\n",
    "from keller_zlatic_vnc.data_processing import generate_standard_id_for_full_annots\n",
    "from keller_zlatic_vnc.data_processing import read_full_annotations\n",
    "from keller_zlatic_vnc.data_processing import whole_brain_extract_dff_with_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = dict()\n",
    "\n",
    "# The file specifying which subjects we should include in the analysis\n",
    "ps['subject_file'] = r'Z:\\bishoplab\\projects\\keller_drive\\keller_vnc\\data\\single_cell\\subjects.csv'\n",
    "\n",
    "# Location of folders containing annotations\n",
    "ps['a4_annot_folder'] = r'Z:\\bishoplab\\projects\\keller_drive\\keller_vnc\\data\\full_annotations\\behavior_csv_cl_A4'\n",
    "ps['a9_annot_folder'] = r'Z:\\bishoplab\\projects\\keller_drive\\keller_vnc\\data\\full_annotations\\behavior_csv_cl_A9'\n",
    "\n",
    "# Location of file containing Chen's annotations - we use this to filter down to only good stimulus events\n",
    "ps['chen_file'] = r'Z:\\bishoplab\\projects\\keller_drive\\keller_vnc\\data\\extracted_dff_v2\\transition_list_CW_11202021.xlsx'\n",
    "\n",
    "# Location of excel file specifying where the data for each experiment is saved relative to the base folder\n",
    "ps['data_loc_file'] = r'Z:\\bishoplab\\projects\\keller_drive\\keller_vnc\\data\\experiment_data_locations.xlsx'\n",
    "\n",
    "# Base folder where datasets are stored \n",
    "ps['dataset_base_folder'] =r'W:\\\\SV4'\n",
    "\n",
    "# Subfolder containing the dataset for each subject\n",
    "ps['dataset_folder'] = 'extracted'\n",
    "\n",
    "# Parameters for declaring preceeding and succeeding quiet behaviors\n",
    "ps['pre_q_th'] = 50\n",
    "ps['succ_q_th'] = 9\n",
    "\n",
    "# Parameters for determing the location of the marked preceeding and succeeding quiet events\n",
    "ps['pre_q_event_l'] = 3 # Event length for the preceeding quiet event\n",
    "ps['succ_q_event_l'] = 3 # Event length for the succeeding quiet event\n",
    "\n",
    "# Data to calculate Delta F/F for in each dataset\n",
    "ps['f_ts_str'] = 'f_1_5_5'\n",
    "ps['bl_ts_str'] = 'bl_1_5_5_long'\n",
    "\n",
    "# Parameters for calculating Delta F/F\n",
    "ps['dff_calc_params'] = dict()\n",
    "ps['dff_calc_params']['background'] = 100\n",
    "ps['dff_calc_params']['ep'] = 20\n",
    "\n",
    "# Parameters for where windows are placed\n",
    "ps['dff_window_type'] = 'start_aligned'\n",
    "ps['dff_window_ref'] = 'beh_after_start'\n",
    "ps['dff_window_offset'] = 0\n",
    "ps['dff_window_length'] = 3\n",
    "ps['dff_window_end_ref'] = None\n",
    "ps['dff_window_end_offset'] = None\n",
    "\n",
    "# Specify where we save our results\n",
    "ps['save_folder'] = r'Z:\\bishoplab\\projects\\keller_drive\\keller_vnc\\results\\new_whole_brain_stats'\n",
    "ps['save_str'] = 'state_dep_1_5_5'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in excel file specifying location of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_fcn(str):\n",
    "    return str.replace(\"'\", \"\")\n",
    "converters = {0:c_fcn, 1:c_fcn}\n",
    "\n",
    "data_locs = pd.read_excel(ps['data_loc_file'], header=1, usecols=[1, 2], converters=converters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find stimulus events for all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SiMView\\anaconda3\\envs\\keller_zlatic_vnc\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Get list of subjects we have annotations for\n",
    "\n",
    "a4_file_paths = glob.glob(str(Path(ps['a4_annot_folder']) / '*.csv'))\n",
    "a9_file_paths = glob.glob(str(Path(ps['a9_annot_folder']) / '*.csv'))\n",
    "\n",
    "n_annot_files = len(a4_file_paths) + len(a9_file_paths)\n",
    "a4_files = np.zeros(n_annot_files, dtype=np.bool)\n",
    "a4_files[0:len(a4_file_paths)] = True\n",
    "\n",
    "annot_file_paths = a4_file_paths + a9_file_paths\n",
    "\n",
    "annot_file_names = [Path(p).name for p in annot_file_paths]\n",
    "annot_subjs = [generate_standard_id_for_full_annots(fn) for fn in annot_file_names]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get stimulus events for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_events = pd.DataFrame()\n",
    "\n",
    "for ind, subj in enumerate(annot_subjs):\n",
    "\n",
    "    # Load the annotations for this subject\n",
    "    tbl = read_full_annotations(annot_file_paths[ind])\n",
    "\n",
    "    # Pull out stimulus events for this subject, noting what comes before and after\n",
    "    stim_tbl = copy.deepcopy(tbl[tbl['beh'] == 'S'])\n",
    "    stim_tbl.insert(0, 'subject_id', subj)\n",
    "    stim_tbl.insert(1, 'event_id', range(stim_tbl.shape[0]))\n",
    "    if a4_files[ind] == True:\n",
    "        stim_tbl.insert(2, 'manipulation_tgt', 'A4')\n",
    "    else:\n",
    "        stim_tbl.insert(2, 'manipulation_tgt', 'A9')\n",
    "    before_after_tbl = find_before_and_after_events(events=stim_tbl, all_events=tbl)\n",
    "    stim_annots = pd.concat([stim_tbl, before_after_tbl], axis=1)\n",
    "    subj_events = subj_events.append(stim_annots, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get rid of any events where we could not classify the type of preceeding or succeeding behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_events = subj_events.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get rid of any stimulus events which are not also in Chen's annotations\n",
    "\n",
    "We do this because some of the stimulus events in the full annotations (Nadine's annotations) should be removed because of artefacts. Chen's annotations only include the stimulus events we should analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing event from: 4359-4364 for subject CW_17-08-23-L4\n",
      "Removing event from: 6004-6009 for subject CW_17-08-23-L4\n",
      "Removing event from: 267-272 for subject CW_17-09-01-L3\n"
     ]
    }
   ],
   "source": [
    "chen_events = read_raw_transitions_from_excel(file=ps['chen_file'])\n",
    "chen_events = chen_events.rename(columns={'Manipulation Start': 'start', 'Manipulation End': 'end'})\n",
    "subj_events = down_select_events(tbl_1=subj_events, tbl_2=chen_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mark preceeding and succeeding quiet events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_before = subj_events['start'] - subj_events['beh_before_end']\n",
    "delta_after = subj_events['beh_after_start'] - subj_events['end']\n",
    "\n",
    "before_quiet_inds = delta_before > ps['pre_q_th']\n",
    "after_quiet_inds = delta_after > ps['succ_q_th']\n",
    "\n",
    "subj_events.loc[before_quiet_inds, 'beh_before'] = 'Q'\n",
    "subj_events.loc[after_quiet_inds, 'beh_after'] = 'Q'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mark the start and stop of the marked quiet events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_before_start = (np.ceil((subj_events[before_quiet_inds]['start'] -\n",
    "                             subj_events[before_quiet_inds]['beh_before_end']) / 2) +\n",
    "                             subj_events[before_quiet_inds]['beh_before_end'])\n",
    "\n",
    "new_before_end = new_before_start + ps['pre_q_event_l'] - 1  # Minus 1 for inclusive indexing\n",
    "subj_events.loc[before_quiet_inds, 'beh_before_start'] = new_before_start\n",
    "subj_events.loc[before_quiet_inds, 'beh_before_end'] = new_before_end\n",
    "\n",
    "new_after_start = (np.ceil((subj_events[after_quiet_inds]['beh_after_start'] -\n",
    "                            subj_events[after_quiet_inds]['end']) / 2) +\n",
    "                            subj_events[after_quiet_inds]['end'])\n",
    "new_after_end = new_after_start + ps['succ_q_event_l'] - 1  # Minus 1 for inclusive indexing\n",
    "subj_events.loc[after_quiet_inds, 'beh_after_start'] = new_after_start\n",
    "subj_events.loc[after_quiet_inds, 'beh_after_end'] = new_after_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract dff along with behavior information from each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing dataset 1 of 65.\n",
      "Done processing dataset 2 of 65.\n",
      "Done processing dataset 3 of 65.\n",
      "Done processing dataset 4 of 65.\n",
      "Done processing dataset 5 of 65.\n",
      "Done processing dataset 6 of 65.\n",
      "Done processing dataset 7 of 65.\n",
      "Done processing dataset 8 of 65.\n",
      "Done processing dataset 9 of 65.\n",
      "Done processing dataset 10 of 65.\n",
      "Done processing dataset 11 of 65.\n",
      "Done processing dataset 12 of 65.\n",
      "Done processing dataset 14 of 65.\n",
      "Done processing dataset 15 of 65.\n",
      "Done processing dataset 16 of 65.\n",
      "Done processing dataset 17 of 65.\n",
      "Done processing dataset 18 of 65.\n",
      "Done processing dataset 19 of 65.\n",
      "Done processing dataset 20 of 65.\n",
      "Done processing dataset 22 of 65.\n",
      "Done processing dataset 23 of 65.\n",
      "Done processing dataset 24 of 65.\n",
      "Done processing dataset 25 of 65.\n",
      "Done processing dataset 29 of 65.\n",
      "Done processing dataset 30 of 65.\n",
      "Done processing dataset 31 of 65.\n",
      "Done processing dataset 32 of 65.\n",
      "Done processing dataset 36 of 65.\n",
      "Done processing dataset 40 of 65.\n",
      "Done processing dataset 41 of 65.\n",
      "Done processing dataset 42 of 65.\n",
      "Done processing dataset 43 of 65.\n",
      "Done processing dataset 44 of 65.\n",
      "Done processing dataset 45 of 65.\n",
      "Done processing dataset 46 of 65.\n",
      "Done processing dataset 47 of 65.\n",
      "Done processing dataset 48 of 65.\n",
      "Done processing dataset 49 of 65.\n",
      "Done processing dataset 50 of 65.\n",
      "Done processing dataset 51 of 65.\n",
      "Done processing dataset 52 of 65.\n",
      "Done processing dataset 53 of 65.\n",
      "Done processing dataset 54 of 65.\n",
      "Done processing dataset 55 of 65.\n",
      "Done processing dataset 56 of 65.\n",
      "Done processing dataset 57 of 65.\n",
      "Done processing dataset 58 of 65.\n",
      "Done processing dataset 59 of 65.\n",
      "Done processing dataset 60 of 65.\n",
      "Done processing dataset 62 of 65.\n",
      "Done processing dataset 63 of 65.\n",
      "Done processing dataset 64 of 65.\n",
      "Done processing dataset 65 of 65.\n"
     ]
    }
   ],
   "source": [
    "analysis_subjects = subj_events['subject_id'].unique()\n",
    "n_datasets = len(data_locs)\n",
    "n_analysis_subjs = len(analysis_subjects)\n",
    "matched_analysis_subjs = np.zeros(n_analysis_subjs)\n",
    "matched_data_subjs = np.zeros(n_datasets)\n",
    "\n",
    "subject_event_data = [None]*n_datasets\n",
    "\n",
    "for d_i in range(n_datasets):\n",
    "    \n",
    "    # ===============================================================================\n",
    "    # Find the appropriate annotations for this experiment\n",
    "    data_main_folder = data_locs['Main folder'][d_i]\n",
    "    data_sub_folder = data_locs['Subfolder'][d_i]\n",
    "\n",
    "    # Parse the subject from the subfolder string\n",
    "    match_ind = match_annotation_subject_to_volume_subject(data_main_folder, data_sub_folder, analysis_subjects)\n",
    "    \n",
    "    if match_ind is not None:\n",
    "        match_ind = match_ind.item()\n",
    "        \n",
    "        \n",
    "        # Prevent any double matching\n",
    "        if matched_analysis_subjs[match_ind] == True:\n",
    "            raise(RuntimeError('Found a transition subject we already matched to: d_i=' + str(d_i)))\n",
    "        if matched_data_subjs[d_i] == True:\n",
    "            raise(RuntimeError('Found a volume subject we already matched to.'))\n",
    "            \n",
    "        matched_analysis_subjs[match_ind] = True\n",
    "        matched_data_subjs[d_i] = True\n",
    "         \n",
    "        # ===============================================================================\n",
    "        # Now that we know we have a unique match, we do the rest of our processing\n",
    "\n",
    "        # Read in the data for this subject\n",
    "        dataset_path = (Path(ps['dataset_base_folder']) / data_main_folder / data_sub_folder / \n",
    "                        Path(ps['dataset_folder']))\n",
    "        dataset_file = glob.glob(str(dataset_path/'*.pkl'))[0]\n",
    "\n",
    "        # Load the dataset\n",
    "        with open(dataset_file, 'rb') as f:\n",
    "            dataset = ROIDataset.from_dict(pickle.load(f))\n",
    "        \n",
    "        # Update the NDArrayHandlers within the dataset object, so they know where to load data from\n",
    "        for ts_label in dataset.ts_data.keys():\n",
    "            ts_vls = dataset.ts_data[ts_label]['vls']\n",
    "            if isinstance(ts_vls, janelia_core.fileio.data_handlers.NDArrayHandler):\n",
    "                ts_vls.folder = str(dataset_path / Path(ts_vls.folder).name)\n",
    "        \n",
    "        # Calculate dff\n",
    "        f=dataset.ts_data[ps['f_ts_str']]['vls'][:]\n",
    "        b=dataset.ts_data[ps['bl_ts_str']]['vls'][:]\n",
    "        dff = calc_dff(f=f, b=b, background=ps['dff_calc_params']['background'], ep=ps['dff_calc_params']['ep'])\n",
    "        \n",
    "        # Extract dff for each event\n",
    "        sample_id = analysis_subjects[match_ind]\n",
    "        event_rows = subj_events['subject_id'] == sample_id\n",
    "        sample_events = copy.deepcopy(subj_events[event_rows])\n",
    "        \n",
    "        subject_event_data[d_i] = whole_brain_extract_dff_with_annotations(dff=dff,\n",
    "                                                              event_tbl=sample_events,\n",
    "                                                              align_col=ps['dff_window_ref'],\n",
    "                                                              ref_offset=ps['dff_window_offset'],\n",
    "                                                              window_l=ps['dff_window_length'],\n",
    "                                                              end_align_col=ps['dff_window_end_ref'],\n",
    "                                                              end_ref_offset=ps['dff_window_end_offset'])\n",
    "        \n",
    "        # Give user some feedback\n",
    "        print('Done processing dataset ' + str(d_i + 1) + ' of ' + str(n_datasets) + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_event_data = pd.concat(subject_event_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what subjects we have transitions for but no registerd volumes\n",
    "ignored_analysis_subjs = analysis_subjects[np.logical_not(matched_analysis_subjs)]\n",
    "\n",
    "# See what subjects we have registered volumes for but no transitions\n",
    "ignored_vol_subjs = data_locs[np.logical_not(matched_data_subjs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: Z:\\bishoplab\\projects\\keller_drive\\keller_vnc\\results\\new_whole_brain_stats\\state_dep_12_60_60.pkl\n"
     ]
    }
   ],
   "source": [
    "## Save results\n",
    "rs = dict()\n",
    "rs['ps'] = ps\n",
    "rs['subject_event_data'] = subject_event_data\n",
    "rs['ignored_analysis_subjs'] = ignored_analysis_subjs\n",
    "rs['ignored_vol_subjs'] = ignored_vol_subjs\n",
    "\n",
    "save_name = ps['save_str'] + '.pkl'\n",
    "save_path = Path(ps['save_folder']) / save_name\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(rs, f)\n",
    "    \n",
    "print('Saved results to: ' + str(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>manipulation_tgt</th>\n",
       "      <th>event_id</th>\n",
       "      <th>beh_before</th>\n",
       "      <th>beh_after</th>\n",
       "      <th>beh</th>\n",
       "      <th>dff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CW_17-08-23-L1</td>\n",
       "      <td>A4</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>P</td>\n",
       "      <td>S</td>\n",
       "      <td>[0.12195438, 0.24093103, 0.15107192, 0.1623803...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CW_17-08-23-L1</td>\n",
       "      <td>A4</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>P</td>\n",
       "      <td>S</td>\n",
       "      <td>[0.102938, 0.18485849, 0.13295324, 0.1565874, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CW_17-08-23-L2</td>\n",
       "      <td>A4</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>B</td>\n",
       "      <td>S</td>\n",
       "      <td>[0.17685132, 0.7027529, 0.19853234, 0.43995404...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CW_17-08-23-L2</td>\n",
       "      <td>A4</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>B</td>\n",
       "      <td>S</td>\n",
       "      <td>[0.084982716, 0.24805646, 0.10991591, 0.225406...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CW_17-08-23-L2</td>\n",
       "      <td>A4</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>B</td>\n",
       "      <td>S</td>\n",
       "      <td>[0.055174757, 0.14208716, 0.08101915, 0.155874...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>CW_17-12-11-L3</td>\n",
       "      <td>A4</td>\n",
       "      <td>3</td>\n",
       "      <td>Q</td>\n",
       "      <td>F</td>\n",
       "      <td>S</td>\n",
       "      <td>[0.23371695, 0.21050055, 0.18734594, 0.2166279...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>CW_17-12-11-L3</td>\n",
       "      <td>A4</td>\n",
       "      <td>4</td>\n",
       "      <td>Q</td>\n",
       "      <td>F</td>\n",
       "      <td>S</td>\n",
       "      <td>[0.15748405, 0.17065044, 0.15362367, 0.1700924...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>CW_17-12-11-L3</td>\n",
       "      <td>A4</td>\n",
       "      <td>5</td>\n",
       "      <td>Q</td>\n",
       "      <td>F</td>\n",
       "      <td>S</td>\n",
       "      <td>[0.1716596, 0.17545824, 0.16065933, 0.18420304...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>CW_17-12-11-L3</td>\n",
       "      <td>A4</td>\n",
       "      <td>6</td>\n",
       "      <td>Q</td>\n",
       "      <td>F</td>\n",
       "      <td>S</td>\n",
       "      <td>[0.15825082, 0.15863396, 0.14227927, 0.1598023...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>CW_17-12-11-L3</td>\n",
       "      <td>A4</td>\n",
       "      <td>7</td>\n",
       "      <td>Q</td>\n",
       "      <td>F</td>\n",
       "      <td>S</td>\n",
       "      <td>[0.08485752, 0.09669999, 0.09784808, 0.1110276...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         subject_id manipulation_tgt event_id beh_before beh_after beh  \\\n",
       "0    CW_17-08-23-L1               A4        0          F         P   S   \n",
       "1    CW_17-08-23-L1               A4        1          F         P   S   \n",
       "2    CW_17-08-23-L2               A4        0          F         B   S   \n",
       "3    CW_17-08-23-L2               A4        1          F         B   S   \n",
       "4    CW_17-08-23-L2               A4        2          F         B   S   \n",
       "..              ...              ...      ...        ...       ...  ..   \n",
       "184  CW_17-12-11-L3               A4        3          Q         F   S   \n",
       "185  CW_17-12-11-L3               A4        4          Q         F   S   \n",
       "186  CW_17-12-11-L3               A4        5          Q         F   S   \n",
       "187  CW_17-12-11-L3               A4        6          Q         F   S   \n",
       "188  CW_17-12-11-L3               A4        7          Q         F   S   \n",
       "\n",
       "                                                   dff  \n",
       "0    [0.12195438, 0.24093103, 0.15107192, 0.1623803...  \n",
       "1    [0.102938, 0.18485849, 0.13295324, 0.1565874, ...  \n",
       "2    [0.17685132, 0.7027529, 0.19853234, 0.43995404...  \n",
       "3    [0.084982716, 0.24805646, 0.10991591, 0.225406...  \n",
       "4    [0.055174757, 0.14208716, 0.08101915, 0.155874...  \n",
       "..                                                 ...  \n",
       "184  [0.23371695, 0.21050055, 0.18734594, 0.2166279...  \n",
       "185  [0.15748405, 0.17065044, 0.15362367, 0.1700924...  \n",
       "186  [0.1716596, 0.17545824, 0.16065933, 0.18420304...  \n",
       "187  [0.15825082, 0.15863396, 0.14227927, 0.1598023...  \n",
       "188  [0.08485752, 0.09669999, 0.09784808, 0.1110276...  \n",
       "\n",
       "[189 rows x 7 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_event_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
