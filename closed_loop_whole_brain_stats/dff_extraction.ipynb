{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to extract and save $\\Delta F / F$ for whole making whole brain maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "#import re\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import janelia_core\n",
    "from janelia_core.dataprocessing.dataset import ROIDataset\n",
    "#from janelia_core.utils.data_saving import append_ts\n",
    "\n",
    "#from keller_zlatic_vnc.data_processing import extract_transitions\n",
    "from keller_zlatic_vnc.data_processing import calc_dff\n",
    "from keller_zlatic_vnc.data_processing import match_annotation_subject_to_volume_subject\n",
    "from keller_zlatic_vnc.data_processing import read_raw_transitions_from_excel\n",
    "#from keller_zlatic_vnc.data_processing import recode_beh\n",
    "from keller_zlatic_vnc.data_processing import down_select_events\n",
    "from keller_zlatic_vnc.data_processing import find_before_and_after_events\n",
    "from keller_zlatic_vnc.data_processing import generate_standard_id_for_full_annots\n",
    "from keller_zlatic_vnc.data_processing import read_full_annotations\n",
    "from keller_zlatic_vnc.data_processing import whole_brain_extract_dff_with_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = dict()\n",
    "\n",
    "# The file specifying which subjects we should include in the analysis\n",
    "ps['subject_file'] = r'Z:\\bishoplab\\projects\\keller_drive\\keller_vnc\\data\\single_cell\\subjects.csv'\n",
    "\n",
    "# Location of folders containing annotations\n",
    "ps['a4_annot_folder'] = r'Z:\\bishoplab\\projects\\keller_drive\\keller_vnc\\data\\full_annotations\\behavior_csv_cl_A4'\n",
    "ps['a9_annot_folder'] = r'Z:\\bishoplab\\projects\\keller_drive\\keller_vnc\\data\\full_annotations\\behavior_csv_cl_A9'\n",
    "\n",
    "# Location of file containing Chen's annotations - we use this to filter down to only good stimulus events\n",
    "ps['chen_file'] = r'Z:\\bishoplab\\projects\\keller_drive\\keller_vnc\\data\\extracted_dff_v2\\transition_list_CW_11202021.xlsx'\n",
    "\n",
    "# Location of excel file specifying where the data for each experiment is saved relative to the base folder\n",
    "ps['data_loc_file'] = r'Z:\\bishoplab\\projects\\keller_drive\\keller_vnc\\data\\experiment_data_locations.xlsx'\n",
    "\n",
    "# Base folder where datasets are stored \n",
    "ps['dataset_base_folder'] =r'W:\\\\SV4'\n",
    "\n",
    "# Subfolder containing the dataset for each subject\n",
    "ps['dataset_folder'] = 'extracted'\n",
    "\n",
    "# Parameters for declaring preceeding and succeeding quiet behaviors\n",
    "ps['pre_q_th'] = 50\n",
    "ps['succ_q_th'] = 9\n",
    "\n",
    "# Parameters for determing the location of the marked preceeding and succeeding quiet events\n",
    "ps['pre_q_event_l'] = 3 # Event length for the preceeding quiet event\n",
    "ps['succ_q_event_l'] = 3 # Event length for the succeeding quiet event\n",
    "\n",
    "# Data to calculate Delta F/F for in each dataset\n",
    "ps['f_ts_str'] = 'f_1_5_5'\n",
    "ps['bl_ts_str'] = 'bl_1_5_5_long'\n",
    "\n",
    "# Parameters for calculating Delta F/F\n",
    "ps['dff_calc_params'] = dict()\n",
    "ps['dff_calc_params']['background'] = 100\n",
    "ps['dff_calc_params']['ep'] = 20\n",
    "\n",
    "# Parameters for where windows are placed\n",
    "ps['dff_window_type'] = 'start_end_aligned'\n",
    "ps['dff_window_ref'] = 'start'\n",
    "ps['dff_window_offset'] = 0\n",
    "ps['dff_window_length'] = None\n",
    "ps['dff_window_end_ref'] = 'end'\n",
    "ps['dff_window_end_offset'] = 1\n",
    "\n",
    "# Specify where we save our results\n",
    "ps['save_folder'] = r'Z:\\bishoplab\\projects\\keller_drive\\keller_vnc\\results\\new_whole_brain_stats\\1_5_5'\n",
    "ps['save_str'] = 'decision_dep_1_5_5'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in excel file specifying location of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_fcn(str):\n",
    "    return str.replace(\"'\", \"\")\n",
    "converters = {0:c_fcn, 1:c_fcn}\n",
    "\n",
    "data_locs = pd.read_excel(ps['data_loc_file'], header=1, usecols=[1, 2], converters=converters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find stimulus events for all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SiMView\\anaconda3\\envs\\keller_zlatic_vnc\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Get list of subjects we have annotations for\n",
    "\n",
    "a4_file_paths = glob.glob(str(Path(ps['a4_annot_folder']) / '*.csv'))\n",
    "a9_file_paths = glob.glob(str(Path(ps['a9_annot_folder']) / '*.csv'))\n",
    "\n",
    "n_annot_files = len(a4_file_paths) + len(a9_file_paths)\n",
    "a4_files = np.zeros(n_annot_files, dtype=np.bool)\n",
    "a4_files[0:len(a4_file_paths)] = True\n",
    "\n",
    "annot_file_paths = a4_file_paths + a9_file_paths\n",
    "\n",
    "annot_file_names = [Path(p).name for p in annot_file_paths]\n",
    "annot_subjs = [generate_standard_id_for_full_annots(fn) for fn in annot_file_names]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get stimulus events for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_events = pd.DataFrame()\n",
    "\n",
    "for ind, subj in enumerate(annot_subjs):\n",
    "\n",
    "    # Load the annotations for this subject\n",
    "    tbl = read_full_annotations(annot_file_paths[ind])\n",
    "\n",
    "    # Pull out stimulus events for this subject, noting what comes before and after\n",
    "    stim_tbl = copy.deepcopy(tbl[tbl['beh'] == 'S'])\n",
    "    stim_tbl.insert(0, 'subject_id', subj)\n",
    "    stim_tbl.insert(1, 'event_id', range(stim_tbl.shape[0]))\n",
    "    if a4_files[ind] == True:\n",
    "        stim_tbl.insert(2, 'manipulation_tgt', 'A4')\n",
    "    else:\n",
    "        stim_tbl.insert(2, 'manipulation_tgt', 'A9')\n",
    "    before_after_tbl = find_before_and_after_events(events=stim_tbl, all_events=tbl)\n",
    "    stim_annots = pd.concat([stim_tbl, before_after_tbl], axis=1)\n",
    "    subj_events = subj_events.append(stim_annots, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get rid of any events where we could not classify the type of preceeding or succeeding behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_events = subj_events.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get rid of any stimulus events which are not also in Chen's annotations\n",
    "\n",
    "We do this because some of the stimulus events in the full annotations (Nadine's annotations) should be removed because of artefacts. Chen's annotations only include the stimulus events we should analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing event from: 4359-4364 for subject CW_17-08-23-L4\n",
      "Removing event from: 6004-6009 for subject CW_17-08-23-L4\n",
      "Removing event from: 267-272 for subject CW_17-09-01-L3\n"
     ]
    }
   ],
   "source": [
    "chen_events = read_raw_transitions_from_excel(file=ps['chen_file'])\n",
    "chen_events = chen_events.rename(columns={'Manipulation Start': 'start', 'Manipulation End': 'end'})\n",
    "subj_events = down_select_events(tbl_1=subj_events, tbl_2=chen_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mark preceeding and succeeding quiet events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_before = subj_events['start'] - subj_events['beh_before_end']\n",
    "delta_after = subj_events['beh_after_start'] - subj_events['end']\n",
    "\n",
    "before_quiet_inds = delta_before > ps['pre_q_th']\n",
    "after_quiet_inds = delta_after > ps['succ_q_th']\n",
    "\n",
    "subj_events.loc[before_quiet_inds, 'beh_before'] = 'Q'\n",
    "subj_events.loc[after_quiet_inds, 'beh_after'] = 'Q'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mark the start and stop of the marked quiet events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_before_start = (np.ceil((subj_events[before_quiet_inds]['start'] -\n",
    "                             subj_events[before_quiet_inds]['beh_before_end']) / 2) +\n",
    "                             subj_events[before_quiet_inds]['beh_before_end'])\n",
    "\n",
    "new_before_end = new_before_start + ps['pre_q_event_l'] - 1  # Minus 1 for inclusive indexing\n",
    "subj_events.loc[before_quiet_inds, 'beh_before_start'] = new_before_start\n",
    "subj_events.loc[before_quiet_inds, 'beh_before_end'] = new_before_end\n",
    "\n",
    "new_after_start = (np.ceil((subj_events[after_quiet_inds]['beh_after_start'] -\n",
    "                            subj_events[after_quiet_inds]['end']) / 2) +\n",
    "                            subj_events[after_quiet_inds]['end'])\n",
    "new_after_end = new_after_start + ps['succ_q_event_l'] - 1  # Minus 1 for inclusive indexing\n",
    "subj_events.loc[after_quiet_inds, 'beh_after_start'] = new_after_start\n",
    "subj_events.loc[after_quiet_inds, 'beh_after_end'] = new_after_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract dff along with behavior information from each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing dataset 1 of 65.\n",
      "Done processing dataset 2 of 65.\n",
      "Done processing dataset 3 of 65.\n",
      "Done processing dataset 4 of 65.\n",
      "Done processing dataset 5 of 65.\n",
      "Done processing dataset 6 of 65.\n",
      "Done processing dataset 7 of 65.\n",
      "Done processing dataset 8 of 65.\n",
      "Done processing dataset 9 of 65.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-3e4934696a8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mts_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'f_ts_str'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'vls'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mts_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bl_ts_str'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'vls'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mdff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_dff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackground\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dff_calc_params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'background'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dff_calc_params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ep'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;31m# Extract dff for each event\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mz:\\bishoplab\\projects\\keller_drive\\keller_vnc\\code\\keller_zlatic_vnc\\keller_zlatic_vnc\\data_processing.py\u001b[0m in \u001b[0;36mcalc_dff\u001b[1;34m(f, b, background, ep)\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[0mdff\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mcalcualted\u001b[0m \u001b[0mdff\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m      \"\"\"\n\u001b[1;32m--> 221\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mbackground\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "analysis_subjects = subj_events['subject_id'].unique()\n",
    "n_datasets = len(data_locs)\n",
    "n_analysis_subjs = len(analysis_subjects)\n",
    "matched_analysis_subjs = np.zeros(n_analysis_subjs)\n",
    "matched_data_subjs = np.zeros(n_datasets)\n",
    "\n",
    "subject_event_data = [None]*n_datasets\n",
    "\n",
    "for d_i in range(n_datasets):\n",
    "    \n",
    "    # ===============================================================================\n",
    "    # Find the appropriate annotations for this experiment\n",
    "    data_main_folder = data_locs['Main folder'][d_i]\n",
    "    data_sub_folder = data_locs['Subfolder'][d_i]\n",
    "\n",
    "    # Parse the subject from the subfolder string\n",
    "    match_ind = match_annotation_subject_to_volume_subject(data_main_folder, data_sub_folder, analysis_subjects)\n",
    "    \n",
    "    if match_ind is not None:\n",
    "        match_ind = match_ind.item()\n",
    "        \n",
    "        \n",
    "        # Prevent any double matching\n",
    "        if matched_analysis_subjs[match_ind] == True:\n",
    "            raise(RuntimeError('Found a transition subject we already matched to: d_i=' + str(d_i)))\n",
    "        if matched_data_subjs[d_i] == True:\n",
    "            raise(RuntimeError('Found a volume subject we already matched to.'))\n",
    "            \n",
    "        matched_analysis_subjs[match_ind] = True\n",
    "        matched_data_subjs[d_i] = True\n",
    "         \n",
    "        # ===============================================================================\n",
    "        # Now that we know we have a unique match, we do the rest of our processing\n",
    "\n",
    "        # Read in the data for this subject\n",
    "        dataset_path = (Path(ps['dataset_base_folder']) / data_main_folder / data_sub_folder / \n",
    "                        Path(ps['dataset_folder']))\n",
    "        dataset_file = glob.glob(str(dataset_path/'*.pkl'))[0]\n",
    "\n",
    "        # Load the dataset\n",
    "        with open(dataset_file, 'rb') as f:\n",
    "            dataset = ROIDataset.from_dict(pickle.load(f))\n",
    "        \n",
    "        # Update the NDArrayHandlers within the dataset object, so they know where to load data from\n",
    "        for ts_label in dataset.ts_data.keys():\n",
    "            ts_vls = dataset.ts_data[ts_label]['vls']\n",
    "            if isinstance(ts_vls, janelia_core.fileio.data_handlers.NDArrayHandler):\n",
    "                ts_vls.folder = str(dataset_path / Path(ts_vls.folder).name)\n",
    "        \n",
    "        # Calculate dff\n",
    "        f=dataset.ts_data[ps['f_ts_str']]['vls'][:]\n",
    "        b=dataset.ts_data[ps['bl_ts_str']]['vls'][:]\n",
    "        dff = calc_dff(f=f, b=b, background=ps['dff_calc_params']['background'], ep=ps['dff_calc_params']['ep'])\n",
    "        \n",
    "        # Extract dff for each event\n",
    "        sample_id = analysis_subjects[match_ind]\n",
    "        event_rows = subj_events['subject_id'] == sample_id\n",
    "        sample_events = copy.deepcopy(subj_events[event_rows])\n",
    "        \n",
    "        subject_event_data[d_i] = whole_brain_extract_dff_with_annotations(dff=dff,\n",
    "                                                              event_tbl=sample_events,\n",
    "                                                              align_col=ps['dff_window_ref'],\n",
    "                                                              window_type=ps['dff_window_type'],\n",
    "                                                              ref_offset=ps['dff_window_offset'],\n",
    "                                                              window_l=ps['dff_window_length'],\n",
    "                                                              end_align_col=ps['dff_window_end_ref'],\n",
    "                                                              end_ref_offset=ps['dff_window_end_offset'])\n",
    "        \n",
    "        # Give user some feedback\n",
    "        print('Done processing dataset ' + str(d_i + 1) + ' of ' + str(n_datasets) + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_event_data = pd.concat(subject_event_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what subjects we have transitions for but no registerd volumes\n",
    "ignored_analysis_subjs = analysis_subjects[np.logical_not(matched_analysis_subjs)]\n",
    "\n",
    "# See what subjects we have registered volumes for but no transitions\n",
    "ignored_vol_subjs = data_locs[np.logical_not(matched_data_subjs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save results\n",
    "rs = dict()\n",
    "rs['ps'] = ps\n",
    "rs['subject_event_data'] = subject_event_data\n",
    "rs['ignored_analysis_subjs'] = ignored_analysis_subjs\n",
    "rs['ignored_vol_subjs'] = ignored_vol_subjs\n",
    "\n",
    "save_name = ps['save_str'] + '.pkl'\n",
    "save_path = Path(ps['save_folder']) / save_name\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(rs, f)\n",
    "    \n",
    "print('Saved results to: ' + str(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_event_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
