{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notebook for the development of code for the production of pain maps (determining neurons which respond to the pain stimulus) for a single subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from scipy.stats import ttest_rel\n",
    "\n",
    "from janelia_core.dataprocessing.dataset import ROIDataset\n",
    "from janelia_core.stats.regression import linear_regression_ols_estimator\n",
    "from janelia_core.stats.regression import grouped_linear_regression_acm_stats\n",
    "from janelia_core.stats.regression import grouped_linear_regression_ols_estimator\n",
    "from janelia_core.stats.regression import grouped_linear_regression_acm_linear_restriction_stats\n",
    "#from janelia_core.stats.permutation_tests import paired_grouped_perm_test\n",
    "\n",
    "from keller_zlatic_vnc.data_processing import calc_dff\n",
    "from keller_zlatic_vnc.data_processing import count_unique_subjs_per_transition\n",
    "from keller_zlatic_vnc.data_processing import count_transitions\n",
    "from keller_zlatic_vnc.data_processing import generate_standard_id_for_full_annots\n",
    "from keller_zlatic_vnc.data_processing import generate_standard_id_for_volume\n",
    "from keller_zlatic_vnc.data_processing import get_basic_clean_annotations_from_full\n",
    "from keller_zlatic_vnc.data_processing import read_full_annotations\n",
    "from keller_zlatic_vnc.whole_brain.pain import _mean_t_test\n",
    "from keller_zlatic_vnc.whole_brain.pain import _mean_perm_test\n",
    "from keller_zlatic_vnc.whole_brain.whole_brain_stat_functions import make_whole_brain_videos_and_max_projs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = dict()\n",
    "\n",
    "# Folders containing a4 and a9 annotation data\n",
    "#ps['annot_folders'] = [r'\\\\dm11\\bishoplab\\projects\\keller_vnc\\data\\full_annotations\\behavior_csv_cl_A4',\n",
    "#                      r'\\\\dm11\\bishoplab\\projects\\keller_vnc\\data\\full_annotations\\behavior_csv_cl_A9',\n",
    "#                      r'\\\\dm11\\bishoplab\\projects\\keller_vnc\\data\\full_annotations\\spontaneous_only_annotations']\n",
    "\n",
    "# Subject we analyze\n",
    "ps['analyze_subj'] = 'CW_18-02-15-L1'\n",
    "\n",
    "ps['annot_folders'] = [r'\\\\dm11\\bishoplab\\projects\\keller_vnc\\data\\full_annotations\\em_volume_behavior_csv']\n",
    "\n",
    "# File containing locations to registered volumes\n",
    "#ps['volume_loc_file'] = r'\\\\dm11\\bishoplab\\projects\\keller_vnc\\data\\experiment_data_locations.xlsx'\n",
    "ps['volume_loc_file'] = r'\\\\dm11\\bishoplab\\projects\\keller_vnc\\data\\EM_volume_experiment_data_locations.xlsx'\n",
    "\n",
    "# List subjects we do not want to include in the analysis\n",
    "ps['exclude_subjs'] = set(['CW_17-11-06-L2'])\n",
    "\n",
    "# Subfolder containing the dataset for each subject\n",
    "ps['dataset_folder'] = 'extracted'\n",
    "\n",
    "# Base folder where datasets are stored \n",
    "ps['dataset_base_folder'] =r'K:\\\\SV4'\n",
    "\n",
    "# Data to calculate Delta F/F for in each dataset\n",
    "ps['f_ts_str'] = 'f_1_5_5'\n",
    "ps['bl_ts_str'] = 'bl_1_5_5_long'\n",
    "\n",
    "# Parameters for calculating dff\n",
    "ps['background'] = 100\n",
    "ps['ep'] = 20\n",
    "\n",
    "# Specify the min and max duration of stimuli for events we include in the analysis\n",
    "ps['min_stim_dur'] = 0\n",
    "ps['max_stim_dur'] = 100\n",
    "\n",
    "# Length of window we pull dff in from before the stimulus\n",
    "ps['n_before_tm_pts'] = 3\n",
    "\n",
    "# Specify if we align the after window to the end of the stimulus or the beginning of the stimulus, can be \n",
    "# either 'start' or 'end'\n",
    "ps['after_aligned'] = 'end'\n",
    "\n",
    "# Offset from the start of the window for dff after the event and the last stimulus timep point.  An offset of 0, \n",
    "# means the first time point in the window will be the last time point the stimulus was delevered\n",
    "ps['after_offset'] = 1 \n",
    "\n",
    "# Length of window we pull dff in from after the stimulus\n",
    "ps['n_after_tm_pts'] = 3 \n",
    "\n",
    "# Specify the type of test we perform.  Can be either 't' or 'perm'\n",
    "ps['test_type'] = 'perm'\n",
    "\n",
    "# Specify the number of permutations to use if we are performing a permutation test\n",
    "ps['n_perms'] = 1000\n",
    "\n",
    "# Folder where we should save results\n",
    "ps['result_folder'] = r'A:\\projects\\keller_vnc\\results\\draft_single_subject_pain_maps'\n",
    "\n",
    "# String to save with file names\n",
    "ps['save_str'] = 'end_aligned_offset_1_t_perm'\n",
    "\n",
    "# Roi group we are using - we need to provide this to the image that makes images and movies\n",
    "ps['roi_group'] = 'rois_1_5_5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of all subjects we can analyze\n",
    "\n",
    "These are those we have registered volumes for and annotations and they are not in the excluded subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all annotation files and the subjects they correspond to\n",
    "annot_file_paths = list(itertools.chain(*[glob.glob(str(Path(folder) / '*.csv')) for folder in ps['annot_folders']]))\n",
    "annot_file_names = [Path(p).name for p in annot_file_paths]\n",
    "annot_subjs = [generate_standard_id_for_full_annots(fn) for fn in annot_file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in location of all registered volumes\n",
    "def c_fcn(str):\n",
    "    return str.replace(\"'\", \"\")\n",
    "converters = {0:c_fcn, 1:c_fcn}\n",
    "\n",
    "volume_locs = pd.read_excel(ps['volume_loc_file'], header=1, usecols=[1, 2], converters=converters)\n",
    "volume_subjs = [generate_standard_id_for_volume(volume_locs.loc[i,'Main folder'], \n",
    "                                                       volume_locs.loc[i,'Subfolder'])  for i in volume_locs.index]\n",
    "volume_inds = [i for i in volume_locs.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine where the annotation and volume data is for the subject we analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_i = np.argwhere(np.asarray(volume_subjs) == ps['analyze_subj'])[0][0]\n",
    "annot_i = np.argwhere(np.asarray(annot_subjs) == ps['analyze_subj'])[0][0]\n",
    "\n",
    "volume_main_folder = volume_locs.loc[volume_inds[volume_i], 'Main folder']\n",
    "volume_sub_folder = volume_locs.loc[volume_inds[volume_i], 'Subfolder']\n",
    "annot_file = annot_file_paths[annot_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the annotation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = read_full_annotations(annot_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Down select to only stimulus events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_inds = [i for i in annotations.index if annotations['beh'][i] == 'S']\n",
    "annotations = annotations.iloc[keep_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = annotations['end'] - annotations['start'] + 1\n",
    "good_durations = (durations >= ps['min_stim_dur']) & (durations <= ps['max_stim_dur'])\n",
    "annotations = annotations[good_durations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13      6\n",
       "69     15\n",
       "117     6\n",
       "163    15\n",
       "198     6\n",
       "240    15\n",
       "270     6\n",
       "306    15\n",
       "334     6\n",
       "348    15\n",
       "377     6\n",
       "403    15\n",
       "422     6\n",
       "424    15\n",
       "458     6\n",
       "461    15\n",
       "487     6\n",
       "496    15\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "durations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we read in the $\\frac{\\Delta F}{F}$ data for the subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gathering neural data for subject.')\n",
    "    \n",
    "dataset_path = (Path(ps['dataset_base_folder']) / volume_main_folder / volume_sub_folder / \n",
    "                Path(ps['dataset_folder']) / '*.pkl')\n",
    "dataset_file = glob.glob(str(dataset_path))[0]\n",
    "    \n",
    "with open(dataset_file, 'rb') as f:\n",
    "    dataset = ROIDataset.from_dict(pickle.load(f))\n",
    "            \n",
    "# Calculate dff\n",
    "f=dataset.ts_data[ps['f_ts_str']]['vls'][:]\n",
    "b=dataset.ts_data[ps['bl_ts_str']]['vls'][:]\n",
    "dff = calc_dff(f=f, b=b, background=ps['background'], ep=ps['ep'])\n",
    "    \n",
    "extracted_dff = dict()\n",
    "for index in annotations.index:\n",
    "    event_start = annotations['start'][index]\n",
    "    event_stop = annotations['end'][index] \n",
    "        \n",
    "    dff_before = np.mean(dff[event_start-ps['n_before_tm_pts']:event_start,:], axis=0)\n",
    "        \n",
    "    if ps['after_aligned'] == 'start':\n",
    "        after_start_ind = event_start + ps['after_offset']\n",
    "    elif ps['after_aligned'] == 'end': \n",
    "        after_start_ind = event_stop + ps['after_offset']\n",
    "    else:\n",
    "        raise('Unable to recogonize value of ps[after_aligned].')\n",
    "    after_stop_ind = after_start_ind + ps['n_after_tm_pts']\n",
    "        \n",
    "    dff_after = np.mean(dff[after_start_ind:after_stop_ind,:], axis=0)\n",
    "\n",
    "    extracted_dff[index] = (dff_before, dff_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove any events where the $\\Delta F /F$ window fell outside of the recorded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_keys = [k for k, vl in extracted_dff.items() if np.all(np.isnan(vl[0]))]\n",
    "print(bad_keys)\n",
    "for key in bad_keys:\n",
    "    del extracted_dff[key]\n",
    "  \n",
    "# Drop same events in annotations, even though we don't use this table anymore, just for good house keeping\n",
    "annotations.drop(bad_keys, axis='index', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff_before = np.stack([extracted_dff[i][0] for i in extracted_dff.keys()])\n",
    "dff_after = np.stack([extracted_dff[i][1] for i in extracted_dff.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rois = dff_before.shape[1]\n",
    "before_dff = [dff_before[:, roi_i] for roi_i in range(n_rois)]\n",
    "after_dff = [dff_after[:, roi_i] for roi_i in range(n_rois)]\n",
    "\n",
    "if __name__ ==  '__main__': \n",
    "    num_processors = multiprocessing.cpu_count()\n",
    "    if num_processors > 1: \n",
    "        num_processors = num_processors - 1 # Leave one processor open for other processing on the machine\n",
    "    pool=multiprocessing.Pool(processes = num_processors)\n",
    "    if ps['test_type'] == 'perm':\n",
    "        print('Performing permtuation tests.')\n",
    "        mn_stats = pool.starmap(_mean_perm_test, zip(before_dff, after_dff, \n",
    "                                                     [ps['n_perms']]*len(before_dff)))\n",
    "    elif ps['test_type'] == 't':\n",
    "        print('Performing t tests.')\n",
    "        mn_stats = pool.starmap(_mean_t_test, zip(before_dff, after_dff))\n",
    "    else:\n",
    "        raise(ValueError('test_type not recogonized'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_vls = np.zeros(n_rois)\n",
    "p_values = np.ones(n_rois) # Default value is 1, which is what we leave if we couldn't calculate a p-value b/c the \n",
    "                           # means before and after stimulus were too close \n",
    "\n",
    "for roi_i in range(n_rois):\n",
    "    \n",
    "    diff_vls[roi_i] = mn_stats[roi_i]['after_mn'] - mn_stats[roi_i]['before_mn']\n",
    "    if not(np.isnan(mn_stats[roi_i]['p'])):  \n",
    "        p_values[roi_i] = mn_stats[roi_i]['p']\n",
    "    else:\n",
    "        p_values[roi_i] = 1.0\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beh_stats = {'pain': {'beta': diff_vls, 'p_values': p_values}}\n",
    "rs = {'beh_stats': beh_stats}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_whole_brain_videos_and_max_projs(rs=rs, save_folder_path=Path(ps['result_folder']),\n",
    "                                      overlay_files=None, save_supp_str=ps['save_str'],\n",
    "                                      gen_mean_movie=False, gen_mean_tiff=False, \n",
    "                                      gen_coef_movies=False, gen_coef_tiffs=False, \n",
    "                                      gen_p_value_movies=False, gen_p_value_tiffs=False, \n",
    "                                      gen_filtered_coef_movies=True, gen_filtered_coef_tiffs=True, \n",
    "                                      gen_combined_movies=False, gen_combined_tiffs=False, \n",
    "                                      gen_combined_projs=False, gen_uber_movies=False, \n",
    "                                      p_vl_thresholds = [.05, .95], \n",
    "                                      ex_dataset_file=dataset_file, \n",
    "                                      roi_group=ps['roi_group'], \n",
    "                                      coef_lims=[0.0, 2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(zip([0, 1, 2], 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
