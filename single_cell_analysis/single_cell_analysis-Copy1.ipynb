{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notebook for prototyping the single cell analysis code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bishopw/Documents/Janelia_Research/Projects/janelia_core/janelia_core/fileio/exp_reader.py:21: UserWarning: Unable to locate pyklb module.  Will not be able to read in .klb files.\n",
      "  warnings.warn('Unable to locate pyklb module.  Will not be able to read in .klb files.')\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import glob\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from janelia_core.dataprocessing.baseline import percentile_filter_1d\n",
    "from janelia_core.stats.regression import grouped_linear_regression_ols_estimator\n",
    "from janelia_core.stats.regression import grouped_linear_regression_acm_stats\n",
    "from janelia_core.stats.regression import grouped_linear_regression_acm_linear_restriction_stats\n",
    "from janelia_core.stats.regression import visualize_coefficient_stats\n",
    "\n",
    "from keller_zlatic_vnc.data_processing import calc_dff\n",
    "from keller_zlatic_vnc.data_processing import count_unique_subjs_per_transition\n",
    "from keller_zlatic_vnc.data_processing import find_before_and_after_events\n",
    "from keller_zlatic_vnc.data_processing import generate_standard_id_for_full_annots\n",
    "from keller_zlatic_vnc.data_processing import read_full_annotations\n",
    "from keller_zlatic_vnc.data_processing import read_trace_data\n",
    "from keller_zlatic_vnc.data_processing import single_cell_extract_dff_with_anotations\n",
    "\n",
    "from keller_zlatic_vnc.linear_modeling import one_hot_from_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters go here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = dict()\n",
    "\n",
    "# The file specifying which subjects we should include in the analysis\n",
    "ps['subject_file'] = r'/Volumes/bishoplab/projects/keller_vnc/data/single_cell/subjects.csv'\n",
    "\n",
    "# Location of files provided by Chen containing the raw fluorescence traces for the single cells\n",
    "ps['trace_base_folder'] = r'/Volumes/bishoplab/projects/keller_vnc/data/single_cell/single_cell_traces'\n",
    "ps['a00c_trace_folder'] = 'A00c'\n",
    "ps['basin_trace_folder'] = 'Basin'\n",
    "ps['handle_trace_folder'] = 'Handle'\n",
    "\n",
    "# Location of folders containing annotations\n",
    "ps['a4_annot_folder'] = r'/Volumes/bishoplab/projects/keller_vnc/data/full_annotations/behavior_csv_cl_A4'\n",
    "ps['a9_annot_folder'] = r'/Volumes/bishoplab/projects/keller_vnc/data/full_annotations/behavior_csv_cl_A9'\n",
    "\n",
    "# Specify the type of neurons we analyze\n",
    "ps['cell_type'] = 'a00c'\n",
    "\n",
    "# Specfy the cell ids we analyze as a list. If None, we analyze all cell ids\n",
    "ps['cell_ids']  = ['antL', 'antR']\n",
    "\n",
    "# Parameters for calculating Delta F/F\n",
    "\n",
    "ps['baseline_calc_params'] = dict()\n",
    "ps['baseline_calc_params']['window_length'] = 30001\n",
    "ps['baseline_calc_params']['filter_start'] = -1500\n",
    "ps['baseline_calc_params']['write_offset'] = 1500\n",
    "ps['baseline_calc_params']['p'] = 0.1\n",
    "\n",
    "ps['dff_calc_params'] = dict()\n",
    "ps['dff_calc_params']['background'] = 100\n",
    "ps['dff_calc_params']['ep'] = 20\n",
    "\n",
    "# Specify which behaviors we are willing to include in the analysis - b/c we have to see each behavior in \n",
    "# enough subjects (see below) all of these behaviors may not be included in an analysis, but this gives the\n",
    "# list of what we are least willing to consider.  If None, all behaviors will be considered\n",
    "\n",
    "ps['behs'] = ['Q', 'TC', 'B', 'F', 'H']\n",
    "\n",
    "# The particular behavior we treat as a reference\n",
    "ps['ref_beh'] = 'Q'\n",
    "\n",
    "# Specify the minimum number of subjects we have to see preceeding and succeeding behaviors in to include in the\n",
    "# analysis\n",
    "ps['min_n_pre_subjs'] = 3\n",
    "\n",
    "# Specify the minimum number of subjects we have to see preceeding and succeeding behaviors in to include in the\n",
    "# analysis\n",
    "ps['min_n_succ_subjs'] = 3\n",
    "\n",
    "# Specify the manipulation target for subjects we want to analyze, None indicates both A4 and A9\n",
    "ps['man_tgt'] = None\n",
    "\n",
    "# Say if we should pool preceeding and succeeding turns\n",
    "ps['pool_pre_turns'] = True\n",
    "ps['pool_succ_turns'] = True\n",
    "\n",
    "# Parameters for declaring preceeding and succeeding quiet behaviors\n",
    "ps['pre_q_th'] = 50\n",
    "ps['succ_q_th'] = 9\n",
    "\n",
    "# The type of window we use\n",
    "ps['dff_window_type'] = 'start_aligned'\n",
    "# The reference we use for aligning windows\n",
    "ps['dff_window_ref'] = 'beh_before_start'\n",
    "\n",
    "# The offset we applying when placing DFF windows\n",
    "ps['dff_window_offset'] = 0\n",
    "\n",
    "# The length of the window we calculate DFF in\n",
    "ps['dff_window_length'] = 3 #3\n",
    "\n",
    "# The event we align the end of the window to (if we are not using windows of fixed length)\n",
    "ps['dff_window_end_ref'] = 'end'\n",
    "\n",
    "# The offset when aligning the end of the window (if we are not using windows of fixed length)\n",
    "ps['dff_window_end_offset'] = 1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the basic data for each subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of all subjects we need to process\n",
    "subjects = list(pd.read_csv(ps['subject_file'])['Subject'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No traces found for handle cells for subject CW_17-08-23-L1.\n",
      "Done reading in data for subject 1 of 64.\n",
      "Done reading in data for subject 2 of 64.\n",
      "Done reading in data for subject 3 of 64.\n",
      "Done reading in data for subject 4 of 64.\n",
      "Done reading in data for subject 5 of 64.\n",
      "Done reading in data for subject 6 of 64.\n",
      "Done reading in data for subject 7 of 64.\n",
      "Done reading in data for subject 8 of 64.\n",
      "Done reading in data for subject 9 of 64.\n",
      "Done reading in data for subject 10 of 64.\n",
      "Done reading in data for subject 11 of 64.\n",
      "Done reading in data for subject 12 of 64.\n",
      "Done reading in data for subject 13 of 64.\n",
      "Done reading in data for subject 14 of 64.\n",
      "Done reading in data for subject 15 of 64.\n",
      "No traces found for handle cells for subject CW_17-08-27-L3.\n",
      "Done reading in data for subject 16 of 64.\n",
      "Done reading in data for subject 17 of 64.\n",
      "Done reading in data for subject 18 of 64.\n",
      "Done reading in data for subject 19 of 64.\n",
      "Done reading in data for subject 20 of 64.\n",
      "Done reading in data for subject 21 of 64.\n",
      "Done reading in data for subject 22 of 64.\n",
      "No traces found for handle cells for subject CW_17-08-31-L1.\n",
      "Done reading in data for subject 23 of 64.\n",
      "No traces found for handle cells for subject CW_17-09-01-L1.\n",
      "Done reading in data for subject 24 of 64.\n",
      "Done reading in data for subject 25 of 64.\n",
      "No traces found for handle cells for subject CW_17-11-02-L3.\n",
      "Done reading in data for subject 26 of 64.\n",
      "Done reading in data for subject 27 of 64.\n",
      "Done reading in data for subject 28 of 64.\n",
      "Done reading in data for subject 29 of 64.\n",
      "Done reading in data for subject 30 of 64.\n",
      "Done reading in data for subject 31 of 64.\n",
      "Done reading in data for subject 32 of 64.\n",
      "Done reading in data for subject 33 of 64.\n",
      "Done reading in data for subject 34 of 64.\n",
      "Done reading in data for subject 35 of 64.\n",
      "Done reading in data for subject 36 of 64.\n",
      "Done reading in data for subject 37 of 64.\n",
      "Done reading in data for subject 38 of 64.\n",
      "Done reading in data for subject 39 of 64.\n",
      "Done reading in data for subject 40 of 64.\n",
      "Done reading in data for subject 41 of 64.\n",
      "Done reading in data for subject 42 of 64.\n",
      "Done reading in data for subject 43 of 64.\n",
      "Done reading in data for subject 44 of 64.\n",
      "Done reading in data for subject 45 of 64.\n",
      "Done reading in data for subject 46 of 64.\n",
      "Done reading in data for subject 47 of 64.\n",
      "Done reading in data for subject 48 of 64.\n",
      "Done reading in data for subject 49 of 64.\n",
      "Done reading in data for subject 50 of 64.\n",
      "No traces found for handle cells for subject CW_17-11-28-L2.\n",
      "Done reading in data for subject 51 of 64.\n",
      "Done reading in data for subject 52 of 64.\n",
      "Done reading in data for subject 53 of 64.\n",
      "Done reading in data for subject 54 of 64.\n",
      "Done reading in data for subject 55 of 64.\n",
      "Done reading in data for subject 56 of 64.\n",
      "Done reading in data for subject 57 of 64.\n",
      "Done reading in data for subject 58 of 64.\n",
      "Done reading in data for subject 59 of 64.\n",
      "Done reading in data for subject 60 of 64.\n",
      "Done reading in data for subject 61 of 64.\n",
      "No traces found for handle cells for subject CW_17-11-30-L3.\n",
      "Done reading in data for subject 62 of 64.\n",
      "Done reading in data for subject 63 of 64.\n",
      "Done reading in data for subject 64 of 64.\n"
     ]
    }
   ],
   "source": [
    "data = type_subjects = read_trace_data(subjects=subjects, \n",
    "                   a00c_trace_folder=Path(ps['trace_base_folder'])/ps['a00c_trace_folder'], \n",
    "                   handle_trace_folder=Path(ps['trace_base_folder'])/ps['handle_trace_folder'], \n",
    "                   basin_trace_folder=Path(ps['trace_base_folder'])/ps['basin_trace_folder'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Down select to only the cells of the type and id we want to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n"
     ]
    }
   ],
   "source": [
    "# Down select by cell type\n",
    "data = data[data['cell_type'] == ps['cell_type']]\n",
    "\n",
    "# Down select by cell id\n",
    "if ps['cell_ids'] is not None:\n",
    "    print('Here')\n",
    "    data = data[data['cell_id'].isin(ps['cell_ids'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate $\\Delta F/F$ for each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cells = data.shape[0]\n",
    "dff = [None]*n_cells\n",
    "for cell_row, cell_idx in enumerate(data.index):\n",
    "    baseline = percentile_filter_1d(data['f'][cell_idx], **ps['baseline_calc_params']) \n",
    "    dff[cell_row] = calc_dff(f=data['f'][cell_idx], b=baseline, **ps['dff_calc_params'])\n",
    "\n",
    "data['dff'] = dff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find stimulus events for the subjects we are analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of subjects we have annotations for\n",
    "a4_file_paths = glob.glob(str(Path(ps['a4_annot_folder']) / '*.csv'))\n",
    "a9_file_paths = glob.glob(str(Path(ps['a9_annot_folder']) / '*.csv'))\n",
    "\n",
    "n_annot_files = len(a4_file_paths) + len(a9_file_paths)\n",
    "a4_files = np.zeros(n_annot_files, dtype=np.bool)\n",
    "a4_files[0:len(a4_file_paths)] = True\n",
    "\n",
    "annot_file_paths = a4_file_paths + a9_file_paths\n",
    "\n",
    "annot_file_names = [Path(p).name for p in annot_file_paths]\n",
    "annot_subjs = [generate_standard_id_for_full_annots(fn) for fn in annot_file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stimulus events for each subject we analyze\n",
    "analysis_subjs = list(data['subject_id'].unique())\n",
    "subj_events = pd.DataFrame()\n",
    "\n",
    "for subj in analysis_subjs:\n",
    "    \n",
    "    # Find the annotations for this subject\n",
    "    ind = np.argwhere(np.asarray(annot_subjs) == subj)\n",
    "    if len(ind) == 0:\n",
    "        raise(RuntimeError('Unable to find annotations for subject ' + subj + '.'))\n",
    "    else:\n",
    "        ind = ind[0][0]\n",
    "        \n",
    "    # Load the annotations for this subject\n",
    "    tbl = read_full_annotations(annot_file_paths[ind])\n",
    "    \n",
    "    # Pull out stimulus events for this subject, noting what comes before and after\n",
    "    stim_tbl = copy.deepcopy(tbl[tbl['beh'] == 'S'])\n",
    "    stim_tbl.insert(0, 'subject_id', subj)\n",
    "    stim_tbl.insert(1, 'event_id', range(stim_tbl.shape[0]))\n",
    "    if a4_files[ind] == True:\n",
    "        stim_tbl.insert(2, 'manipulation_tgt', 'A4')\n",
    "    else:\n",
    "        stim_tbl.insert(2, 'manipulation_tgt', 'A9')\n",
    "    before_after_tbl = find_before_and_after_events(events=stim_tbl, all_events=tbl)\n",
    "    stim_annots = pd.concat([stim_tbl, before_after_tbl], axis=1)\n",
    "    subj_events = subj_events.append(stim_annots, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get rid of any events where we could not classify the type of preceeding or succeeding behavior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_events = subj_events.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mark preceeding and succeeding quiet events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_before = subj_events['start'] - subj_events['beh_before_end']\n",
    "delta_after = subj_events['beh_after_start'] - subj_events['end']\n",
    "\n",
    "subj_events.loc[delta_before > ps['pre_q_th'], 'beh_before'] = 'Q'\n",
    "subj_events.loc[delta_after > ps['succ_q_th'], 'beh_after'] = 'Q'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Down select events based on manipulation target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ps['man_tgt'] is not None:\n",
    "    subj_events = subj_events[subj_events['manipulation_tgt'] == ps['man_tgt']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pool turns if we are suppose to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ps['pool_pre_turns']:\n",
    "    turn_rows = (subj_events['beh_before'] == 'TL') | (subj_events['beh_before'] == 'TR')\n",
    "    subj_events.loc[turn_rows, 'beh_before'] = 'TC'\n",
    "\n",
    "if ps['pool_succ_turns']:\n",
    "    turn_rows = (subj_events['beh_after'] == 'TL') | (subj_events['beh_after'] == 'TR')\n",
    "    subj_events.loc[turn_rows, 'beh_after'] = 'TC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Down select to only the type of behaviors we are willing to consider "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ps['behs'] is not None:\n",
    "    keep_inds = [i for i in subj_events.index if subj_events['beh_before'][i] in set(ps['behs'])]\n",
    "    subj_events = subj_events.loc[keep_inds]\n",
    "    \n",
    "    keep_inds = [i for i in subj_events.index if subj_events['beh_after'][i] in set(ps['behs'])]\n",
    "    subj_events = subj_events.loc[keep_inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop any behaviors that do not appear in enough subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_trans_counts = count_unique_subjs_per_transition(table=subj_events)\n",
    "n_before_subjs = subj_trans_counts.sum(axis=1)\n",
    "n_after_subjs = subj_trans_counts.sum(axis=0)\n",
    "\n",
    "before_an_behs = set([i for i in n_before_subjs.index if n_before_subjs[i] >= ps['min_n_pre_subjs']])\n",
    "after_an_behs = set([i for i in n_after_subjs.index if n_after_subjs[i] >= ps['min_n_succ_subjs']])\n",
    "\n",
    "keep_inds = [i for i in subj_events.index if subj_events['beh_before'][i] in before_an_behs]\n",
    "subj_events = subj_events.loc[keep_inds]\n",
    "\n",
    "keep_inds = [i for i in subj_events.index if subj_events['beh_after'][i] in after_an_behs]\n",
    "subj_events = subj_events.loc[keep_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull out $\\Delta F /F$ for each event and cell along with all information we need for performing statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_tbl = single_cell_extract_dff_with_anotations(activity_tbl=data, event_tbl=subj_events,\n",
    "                                                   align_col=ps['dff_window_ref'],\n",
    "                                                   ref_offset=ps['dff_window_offset'],\n",
    "                                                   window_l=ps['dff_window_length'],\n",
    "                                                   window_type=ps['dff_window_type'],\n",
    "                                                   end_align_col=ps['dff_window_end_ref'],\n",
    "                                                   end_ref_offset=ps['dff_window_end_offset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find grouping of data by subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids = full_tbl['subject_id'].unique()\n",
    "g = np.zeros(len(full_tbl))\n",
    "for u_i, u_id in enumerate(unique_ids):\n",
    "    g[full_tbl['subject_id'] == u_id] = u_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit models and calculate stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_behs = full_tbl['beh_before'].unique() \n",
    "before_behs_ref = list(set(before_behs).difference(ps['ref_beh']))\n",
    "\n",
    "after_behs = full_tbl['beh_after'].unique() \n",
    "after_behs_ref = list(set(after_behs).difference(ps['ref_beh']))\n",
    "\n",
    "\n",
    "one_hot_data_ref, one_hot_vars_ref = one_hot_from_table(full_tbl, beh_before=before_behs_ref, \n",
    "                                                        beh_after=after_behs_ref)\n",
    "\n",
    "one_hot_data_ref = np.concatenate([one_hot_data_ref, np.ones([one_hot_data_ref.shape[0], 1])], axis=1)\n",
    "one_hot_vars_ref = one_hot_vars_ref + ['ref'] \n",
    "\n",
    "_, v, _ = np.linalg.svd(one_hot_data_ref)\n",
    "if np.min(v) < .001:\n",
    "    raise(RuntimeError('regressors are nearly co-linear'))\n",
    "    \n",
    "beta, acm, n_gprs = grouped_linear_regression_ols_estimator(x=one_hot_data_ref, y=full_tbl['dff'].to_numpy(), \n",
    "                                                                g=g)\n",
    "\n",
    "mdl_stats = grouped_linear_regression_acm_stats(beta=beta, acm=acm, n_grps=n_gprs, alpha=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we compare coefficients\n",
    "n_grps = len(np.unique(g))\n",
    "\n",
    "cmp_vars = one_hot_vars_ref[0:-1]\n",
    "cmp_p_vls = np.zeros(len(cmp_vars))\n",
    "\n",
    "\n",
    "before_inds = np.asarray([True if re.match('beh_before*', var) else False for var in one_hot_vars_ref])\n",
    "after_inds = np.asarray([True if re.match('beh_after*', var) else False for var in one_hot_vars_ref])\n",
    "\n",
    "n_before_vars = np.sum(before_inds)\n",
    "n_after_vars = np.sum(after_inds)\n",
    "\n",
    "for v_i, var in enumerate(cmp_vars):\n",
    "    if before_inds[v_i] == True:\n",
    "        cmp_beta = beta[before_inds]\n",
    "        cmp_acm = acm[np.ix_(before_inds, before_inds)]\n",
    "        cmp_i = v_i\n",
    "    else:\n",
    "        cmp_beta = beta[after_inds]\n",
    "        cmp_acm = acm[np.ix_(after_inds, after_inds)]\n",
    "        cmp_i = v_i - n_before_vars\n",
    "        \n",
    "    r = np.ones(len(cmp_beta))/(len(cmp_beta) - 1)\n",
    "    r[cmp_i] = -1\n",
    "    cmp_p_vls[v_i] = grouped_linear_regression_acm_linear_restriction_stats(beta=cmp_beta, acm=cmp_acm, r=r,\n",
    "                                                                   q=np.asarray([0]), n_grps=n_grps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_coefficient_stats(var_strs=one_hot_vars_ref, theta=beta, c_ints=mdl_stats['c_ints'], \n",
    "                            sig=mdl_stats['non_zero'],\n",
    "                            x_axis_rot=90)\n",
    "plt.ylabel('$\\Delta F / F$')\n",
    "plt.xlabel('Behavior')\n",
    "plt.tight_layout()\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(8, 6)\n",
    "\n",
    "for v_i, var in enumerate(cmp_vars):\n",
    "    print(var + ': ' + str(cmp_p_vls[v_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(data[data['subject_id'] == 'CW_17-11-03-L6-2']['dff'][1755])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_events[subj_events['subject_id'] == 'CW_17-11-03-L6-2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(subj_events['end'] - subj_events['start']).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
