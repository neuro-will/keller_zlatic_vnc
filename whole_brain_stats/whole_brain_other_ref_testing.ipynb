{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests for state, decision or reporting dependence in DFF, referencing each behavior to the \"other\" condition, across all voxels\n",
    "in the brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from janelia_core.utils.data_saving import append_ts\n",
    "from janelia_core.stats.regression import grouped_linear_regression_ols_estimator\n",
    "from janelia_core.stats.regression import grouped_linear_regression_acm_stats\n",
    "\n",
    "from keller_zlatic_vnc.data_processing import count_unique_subjs_per_transition\n",
    "from keller_zlatic_vnc.data_processing import extract_transitions\n",
    "from keller_zlatic_vnc.linear_modeling import one_hot_from_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = {}\n",
    "\n",
    "# Specigy where the processed data (the produced of dff_extraction.ipynb) is stored\n",
    "ps['data_folder'] = r'A:\\projects\\keller_vnc\\results\\whole_brain_stats'\n",
    "ps['data_file'] = r'dff_1_5_5_with_ep_2020_08_27_12_32_48_526097.pkl'\n",
    "\n",
    "# Speccify a cut-off time\n",
    "ps['cut_off_time'] = 3.656\n",
    "\n",
    "# Specify manipulation target\n",
    "ps['manip_type'] = 'A4' # 'both', 'A4' or 'A9'\n",
    "\n",
    "# Specify thresholds for number of subjects we need for each behavior\n",
    "ps['min_n_subjects_per_beh'] = 3\n",
    "\n",
    "# Specify the test type  Options are:\n",
    "#\n",
    "#   state_dependence - tests if dff after manipulation is sensitive to behavior before\n",
    "#   prediction_dependence - tests if dff before manipulation is sensitive to behavior after\n",
    "#   decision_dependence - tests if dff during manipulation is sensitive to behavior after\n",
    "#   before_reporting - tests if dff before manipulation is sensitive to behavior before\n",
    "#   after_reporting - tests if dff after manipulation is sensitive to behavior after\n",
    "#\n",
    "ps['test_type'] = 'prediction_dependence'\n",
    "\n",
    "# Specify reference behavior for the control behaviors - this will not affect the results for test behaviors\n",
    "ps['beh_ref'] = 'Q'\n",
    "\n",
    "# Alpha value for thresholding significance\n",
    "ps['alpha'] = .05\n",
    "\n",
    "# Specify where we save results\n",
    "ps['save_folder'] = r'\\\\dm11\\bishoplab\\projects\\keller_vnc\\results\\whole_brain_stats'\n",
    "ps['save_str'] = 'other_ref_A4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(ps['data_folder']) / Path(ps['data_file'])\n",
    "with open(data_path, 'rb') as f:\n",
    "    file_data = pickle.load(f)\n",
    "data = file_data['event_annots']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename a few columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {'Smp ID':'subject_id', 'Beh Before':'beh_before', 'Beh After':'beh_after'}, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply cut-off time to define succeeding quiet behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, data = extract_transitions(data, ps['cut_off_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Down select for manipulation target if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ps['manip_type'] == 'A4':\n",
    "    data = data[data['Tgt Site'] == 'A4']\n",
    "elif ps['manip_type'] == 'A9':\n",
    "    data = data[data['Tgt Site'] == 'A9']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep only events with behaviors that are present in enough subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_subj_cnts = count_unique_subjs_per_transition(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (ps['test_type'] == 'state_dependence') or (ps['test_type'] == 'before_reporting'):\n",
    "    after_beh_th = 0\n",
    "    before_beh_th = ps['min_n_subjects_per_beh']\n",
    "elif ((ps['test_type'] == 'prediction_dependence') or (ps['test_type'] == 'after_reporting') or \n",
    "      (ps['test_type'] == 'decision_dependence')):\n",
    "    after_beh_th = ps['min_n_subjects_per_beh']\n",
    "    before_beh_th = 0\n",
    "else:\n",
    "    raise(ValueError('The test_type ' + ps['test_type'] + ' is not recognized.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_beh_sum = trans_subj_cnts.sum()\n",
    "after_behs = [b for b in after_beh_sum[after_beh_sum > after_beh_th].index]\n",
    "\n",
    "before_beh_sum = trans_subj_cnts.sum(1)\n",
    "before_behs = [b for b in before_beh_sum[before_beh_sum > before_beh_th].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_events = data['beh_before'].isin(set(before_behs)) & data['beh_after'].isin(set(after_behs))\n",
    "data = data[keep_events]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the test and control behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting test behaviors to those after the manipulation.\n"
     ]
    }
   ],
   "source": [
    "if (ps['test_type'] == 'state_dependence') or (ps['test_type'] == 'before_reporting'):\n",
    "    test_behs = before_behs\n",
    "    control_behs = after_behs\n",
    "    print('Setting test behaviors to those before the manipulation.')\n",
    "elif ((ps['test_type'] == 'prediction_dependence') or (ps['test_type'] == 'after_reporting') or \n",
    "      (ps['test_type'] == 'decision_dependence')):\n",
    "    test_behs = after_behs\n",
    "    control_behs = before_behs\n",
    "    print('Setting test behaviors to those after the manipulation.')\n",
    "else:\n",
    "    raise(ValueError('The test_type ' + ps['test_type'] + ' is not recognized.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get groups of subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids = data['subject_id'].unique()\n",
    "g = np.zeros(len(data))\n",
    "for u_i, u_id in enumerate(unique_ids):\n",
    "    g[data['subject_id'] == u_id] = u_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull out $\\Delta F/F$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dff before the manipulation.\n"
     ]
    }
   ],
   "source": [
    "if (ps['test_type'] == 'state_dependence') or (ps['test_type'] == 'after_reporting'):\n",
    "    dff = np.stack(data['dff_after'].to_numpy())\n",
    "    print('Extracting dff after the manipulation.')\n",
    "elif (ps['test_type'] == 'prediction_dependence') or (ps['test_type'] == 'before_reporting'):\n",
    "    dff = np.stack(data['dff_before'].to_numpy())\n",
    "    print('Extracting dff before the manipulation.')\n",
    "elif ps['test_type'] == 'decision_dependence':\n",
    "    dff = np.stack(data['dff_during'].to_numpy())\n",
    "    print('Extracting dff during the manipulation.')\n",
    "else:\n",
    "    raise(ValueError('The test_type ' + ps['test_type'] + ' is not recognized.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function for calculating stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_f(x_i, y_i, g_i, alpha_i):\n",
    "    beta, acm, n_grps = grouped_linear_regression_ols_estimator(x=x_i, y=y_i, g=g_i)\n",
    "    stats = grouped_linear_regression_acm_stats(beta=beta, acm=acm, n_grps=n_grps, alpha=alpha_i)\n",
    "    stats['beta'] = beta\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests for behavior 1 of 5: B\n",
      "Done.  Elapsed time: 1489.9820528030396\n",
      "Running tests for behavior 2 of 5: F\n",
      "Done.  Elapsed time: 2267.4333856105804\n",
      "Running tests for behavior 3 of 5: O\n",
      "Done.  Elapsed time: 3024.3037209510803\n",
      "Running tests for behavior 4 of 5: P\n",
      "Done.  Elapsed time: 3779.1211915016174\n",
      "Running tests for behavior 5 of 5: Q\n",
      "Done.  Elapsed time: 4548.0077204704285\n"
     ]
    }
   ],
   "source": [
    "n_rois = dff.shape[1]\n",
    "n_test_behs = len(test_behs)\n",
    "full_stats = dict()\n",
    "t0 = time()\n",
    "for b_i, b in enumerate(test_behs):\n",
    "    print('Running tests for behavior ' + str(b_i + 1) + ' of ' + str(n_test_behs) + ': ' + b)\n",
    "    \n",
    "    control_behs_ref = list(set(control_behs).difference(ps['beh_ref']))\n",
    "    \n",
    "    if (ps['test_type'] == 'state_dependence') or (ps['test_type'] == 'before_reporting'):\n",
    "        one_hot_data_ref, one_hot_vars_ref = one_hot_from_table(data, beh_before=[b], beh_after=control_behs_ref)\n",
    "        pull_ind = 0\n",
    "    elif ((ps['test_type'] == 'prediction_dependence') or (ps['test_type'] == 'after_reporting') or\n",
    "         (ps['test_type'] == 'decision_dependence')):\n",
    "        one_hot_data_ref, one_hot_vars_ref = one_hot_from_table(data, beh_before=control_behs_ref, beh_after=[b])\n",
    "        pull_ind = len(one_hot_vars_ref)-1\n",
    "    else:\n",
    "        raise(ValueError('The test_type ' + ps['test_type'] + ' is not recognized.'))\n",
    "        \n",
    "    one_hot_data_ref = np.concatenate([one_hot_data_ref, np.ones([one_hot_data_ref.shape[0], 1])], axis=1)\n",
    "    one_hot_vars_ref = one_hot_vars_ref + ['ref']\n",
    "    \n",
    "    full_stats[b] = [(stats_f(x_i=one_hot_data_ref, y_i=dff[:, r_i], g_i=g, alpha_i=ps['alpha']), pull_ind) \n",
    "                 for r_i in range(n_rois)]\n",
    "    \n",
    "    print('Done.  Elapsed time: ' + str(time() - t0))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "beh_stats = dict()\n",
    "for b in test_behs:\n",
    "    beh_stats[b] = dict()\n",
    "    beh_stats[b]['p_values'] = [rs_dict['non_zero_p'][rs_pull_ind]\n",
    "                                for (rs_dict, rs_pull_ind) in full_stats[b]]\n",
    "    beh_stats[b]['beta'] = [rs_dict['beta'][rs_pull_ind]\n",
    "                                for (rs_dict, rs_pull_ind) in full_stats[b]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = append_ts(ps['test_type'] + '_' + ps['save_str']) + '.pkl'\n",
    "save_path = Path(ps['save_folder']) / save_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = dict()\n",
    "rs['beh_stats'] = beh_stats\n",
    "rs['full_stats'] = full_stats\n",
    "rs['ps'] = ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(rs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: \\\\dm11\\bishoplab\\projects\\keller_vnc\\results\\whole_brain_stats\\prediction_dependence_other_ref_A4_2020_09_26_22_54_03_390269.pkl\n"
     ]
    }
   ],
   "source": [
    "print('Saved results to: ' + str(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
