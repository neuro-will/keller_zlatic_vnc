{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to extract and save $\\Delta F / F$ for whole making whole brain maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from janelia_core.dataprocessing.dataset import ROIDataset\n",
    "from janelia_core.utils.data_saving import append_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = dict()\n",
    "\n",
    "# Location of excel file specifying where the data for each experiment is saved relative to the base folder\n",
    "ps['data_loc_file'] = r'A:\\projects\\keller_vnc\\data\\experiment_data_locations.xlsx'\n",
    "\n",
    "# Subfolder containing the dataset for each subject\n",
    "ps['dataset_folder'] = 'extracted'\n",
    "\n",
    "# Base folder where datasets are stored \n",
    "ps['dataset_base_folder'] =r'K:\\\\SV4'\n",
    "\n",
    "# Data to calculate Delta F/F for in each dataset\n",
    "ps['f_ts_str'] = 'f_5_25_25'\n",
    "ps['bl_ts_str'] = 'bl_5_25_25'\n",
    "\n",
    "ps['n_smps'] = 3 # Number of samples to take before and after a manipulation \n",
    "\n",
    "# Parameters for calculating dff\n",
    "ps['background'] = 100\n",
    "ps['ep'] = 20\n",
    "\n",
    "# Location to save results\n",
    "ps['save_folder'] = r'A:\\projects\\keller_vnc\\results\\whole_brain_stats'\n",
    "ps['save_str'] = 'dff_5_25_25_with_ep'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in excel file specifying location of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_fcn(str):\n",
    "    return str.replace(\"'\", \"\")\n",
    "converters = {0:c_fcn, 1:c_fcn}\n",
    "\n",
    "data_locs = pd.read_excel(ps['data_loc_file'], header=1, usecols=[1, 2], converters=converters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function to calculate $\\Delta F/F$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dff(f, b, background=ps['background'], ep=ps['ep']):\n",
    "    return (f-b)/(b-background+ep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract dff along with behavior information from each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min b: 48.071285\n",
      "Done processing dataset 1 of 66.\n",
      "min b: 102.64291\n",
      "Done processing dataset 2 of 66.\n",
      "min b: 101.983955\n",
      "Done processing dataset 3 of 66.\n",
      "min b: 89.34743\n",
      "Done processing dataset 4 of 66.\n",
      "min b: 4.5419083\n",
      "Done processing dataset 5 of 66.\n",
      "min b: 91.72084\n",
      "Done processing dataset 6 of 66.\n",
      "min b: 106.12106\n",
      "Done processing dataset 7 of 66.\n",
      "min b: 26.00942\n",
      "Done processing dataset 8 of 66.\n",
      "min b: 101.25182\n",
      "Done processing dataset 9 of 66.\n",
      "min b: 89.86631\n",
      "Done processing dataset 10 of 66.\n",
      "min b: 42.518364\n",
      "Done processing dataset 11 of 66.\n",
      "min b: 91.78164\n",
      "Done processing dataset 12 of 66.\n",
      "min b: 84.09773\n",
      "Done processing dataset 13 of 66.\n",
      "min b: 74.68948\n",
      "Done processing dataset 14 of 66.\n",
      "min b: 45.780064\n",
      "Done processing dataset 15 of 66.\n",
      "min b: 98.6193\n",
      "Done processing dataset 16 of 66.\n",
      "min b: 84.07614\n",
      "Done processing dataset 17 of 66.\n",
      "min b: 104.29418\n",
      "Done processing dataset 18 of 66.\n",
      "min b: 0.0\n",
      "Done processing dataset 19 of 66.\n",
      "min b: 0.0\n",
      "Done processing dataset 21 of 66.\n",
      "min b: 0.0\n",
      "Done processing dataset 22 of 66.\n",
      "min b: 32.07251\n",
      "Done processing dataset 23 of 66.\n",
      "min b: 0.0\n",
      "Done processing dataset 24 of 66.\n",
      "min b: 17.380888\n",
      "Done processing dataset 25 of 66.\n",
      "min b: 51.616745\n",
      "Done processing dataset 27 of 66.\n",
      "min b: 47.820717\n",
      "Done processing dataset 28 of 66.\n",
      "min b: 30.16767\n",
      "Done processing dataset 29 of 66.\n",
      "min b: 66.23417\n",
      "Done processing dataset 30 of 66.\n",
      "min b: 0.0\n",
      "Done processing dataset 31 of 66.\n",
      "min b: 0.0\n",
      "Done processing dataset 32 of 66.\n",
      "min b: 102.3438\n",
      "Done processing dataset 33 of 66.\n",
      "min b: 104.20667\n",
      "Done processing dataset 34 of 66.\n",
      "min b: 94.38774\n",
      "Done processing dataset 37 of 66.\n",
      "min b: 81.6081\n",
      "Done processing dataset 38 of 66.\n",
      "min b: 92.57022\n",
      "Done processing dataset 39 of 66.\n",
      "min b: 103.31321\n",
      "Done processing dataset 40 of 66.\n",
      "min b: 74.299065\n",
      "Done processing dataset 41 of 66.\n",
      "min b: 96.418724\n",
      "Done processing dataset 42 of 66.\n",
      "min b: 107.552124\n",
      "Done processing dataset 43 of 66.\n",
      "min b: 62.408108\n",
      "Done processing dataset 44 of 66.\n",
      "min b: 66.90882\n",
      "Done processing dataset 45 of 66.\n",
      "min b: 65.30701\n",
      "Done processing dataset 46 of 66.\n",
      "min b: 36.361835\n",
      "Done processing dataset 47 of 66.\n",
      "min b: 85.672585\n",
      "Done processing dataset 48 of 66.\n",
      "min b: 89.1419\n",
      "Done processing dataset 49 of 66.\n",
      "min b: 87.35909\n",
      "Done processing dataset 50 of 66.\n",
      "min b: 61.3861\n",
      "Done processing dataset 51 of 66.\n",
      "min b: 104.30129\n",
      "Done processing dataset 52 of 66.\n",
      "min b: 99.64232\n",
      "Done processing dataset 53 of 66.\n",
      "min b: 103.793846\n",
      "Done processing dataset 54 of 66.\n",
      "min b: 102.85674\n",
      "Done processing dataset 55 of 66.\n",
      "min b: 101.28687\n",
      "Done processing dataset 56 of 66.\n",
      "min b: 64.47019\n",
      "Done processing dataset 57 of 66.\n",
      "min b: 73.58151\n",
      "Done processing dataset 58 of 66.\n",
      "min b: 102.45121\n",
      "Done processing dataset 59 of 66.\n",
      "min b: 103.80418\n",
      "Done processing dataset 60 of 66.\n",
      "min b: 0.0\n",
      "Done processing dataset 61 of 66.\n",
      "min b: 96.476944\n",
      "Done processing dataset 62 of 66.\n",
      "min b: 0.0\n",
      "Done processing dataset 63 of 66.\n",
      "min b: 37.60507\n",
      "Done processing dataset 64 of 66.\n",
      "min b: 103.30928\n",
      "Done processing dataset 65 of 66.\n",
      "min b: 104.6817\n",
      "Done processing dataset 66 of 66.\n"
     ]
    }
   ],
   "source": [
    "n_datasets = len(data_locs)\n",
    "event_annots = [None]*n_datasets\n",
    "\n",
    "for d_i in range(n_datasets):\n",
    "    # Get path to dataset\n",
    "    dataset_path = (Path(ps['dataset_base_folder']) / data_locs['Main folder'][d_i] / data_locs['Subfolder'][d_i] / \n",
    "                    Path(ps['dataset_folder']) / '*.pkl')\n",
    "    dataset_file = glob.glob(str(dataset_path))\n",
    "    if len(dataset_file) != 0:\n",
    "        dataset_file = dataset_file[0]\n",
    "\n",
    "        # Load the dataset\n",
    "        with open(dataset_file, 'rb') as f:\n",
    "            dataset = ROIDataset.from_dict(pickle.load(f))\n",
    "    \n",
    "        # Calculate dff\n",
    "        f=dataset.ts_data[ps['f_ts_str']]['vls'][:]\n",
    "        b=dataset.ts_data[ps['bl_ts_str']]['vls'][:]\n",
    "        dff = calc_dff(f=f, b=b)\n",
    "        \n",
    "        # Extract dff before and after each manipulation event and store it \n",
    "        event_annot = dataset.metadata['manip_event_annotations']\n",
    "        n_events = len(event_annot)\n",
    "        dff_before = [None]*n_events\n",
    "        dff_after = [None]*n_events\n",
    "        for e_i in range(n_events):\n",
    "            m_start = event_annot['Manipulation Start'].to_numpy()[e_i]\n",
    "            m_end = event_annot['Manipulation End'].to_numpy()[e_i]\n",
    "            dff_before[e_i] = np.mean(dff[m_start-3:m_start,:], axis=0)\n",
    "            dff_after[e_i] = np.mean(dff[m_end+1:m_end+4,:], axis=0)\n",
    "\n",
    "        event_annot['dff_before'] = dff_before\n",
    "        event_annot['dff_after'] = dff_after\n",
    "    \n",
    "        # Save results\n",
    "        event_annots[d_i] = event_annot\n",
    "    \n",
    "        # Give user some feedback\n",
    "        print('Done processing dataset ' + str(d_i + 1) + ' of ' + str(n_datasets) + '.')\n",
    "\n",
    "# Place event annotations into a single table\n",
    "event_annots = pd.concat(event_annots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: A:\\projects\\keller_vnc\\results\\whole_brain_stats\\dff_5_25_25_with_ep_2019_10_29_15_22_01_205757.pkl\n"
     ]
    }
   ],
   "source": [
    "## Save results\n",
    "rs = dict()\n",
    "rs['ps'] = ps\n",
    "rs['event_annots'] = event_annots\n",
    "\n",
    "save_name = append_ts(ps['save_str']) + '.pkl'\n",
    "save_path = Path(ps['save_folder']) / save_name\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(rs, f)\n",
    "    \n",
    "print('Saved results to: ' + str(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=dataset.ts_data[ps['f_ts_str']]['vls'][:]\n",
    "b=dataset.ts_data[ps['bl_ts_str']]['vls'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104.6817"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36431351956896996"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 - 1/52)**52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
