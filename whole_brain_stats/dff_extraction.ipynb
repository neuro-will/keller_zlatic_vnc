{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to extract and save $\\Delta F / F$ for whole making whole brain maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from janelia_core.dataprocessing.dataset import ROIDataset\n",
    "from janelia_core.utils.data_saving import append_ts\n",
    "\n",
    "from keller_zlatic_vnc.data_processing import extract_transitions\n",
    "from keller_zlatic_vnc.data_processing import match_annotation_subject_to_volume_subject\n",
    "from keller_zlatic_vnc.data_processing import read_raw_transitions_from_excel\n",
    "from keller_zlatic_vnc.data_processing import recode_beh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = dict()\n",
    "\n",
    "# Location of excel file specifying where the data for each experiment is saved relative to the base folder\n",
    "ps['data_loc_file'] = r'A:\\projects\\keller_vnc\\data\\experiment_data_locations.xlsx'\n",
    "\n",
    "# Location of excel file specifying transition information \n",
    "ps['trans_file'] = r'A:\\projects\\keller_vnc\\data\\extracted_dff_v2\\transition_list.xlsx'\n",
    "\n",
    "# Subfolder containing the dataset for each subject\n",
    "ps['dataset_folder'] = 'extracted'\n",
    "\n",
    "# Base folder where datasets are stored \n",
    "ps['dataset_base_folder'] =r'K:\\\\SV4'\n",
    "\n",
    "# Data to calculate Delta F/F for in each dataset\n",
    "ps['f_ts_str'] = 'f_12_60_60'\n",
    "ps['bl_ts_str'] = 'bl_12_60_60_long'\n",
    "\n",
    "ps['n_smps'] = 3 # Number of samples to take before and after a manipulation \n",
    "\n",
    "# Parameters for calculating dff\n",
    "ps['background'] = 100\n",
    "ps['ep'] = 20\n",
    "\n",
    "# Location to save results\n",
    "ps['save_folder'] = r'A:\\projects\\keller_vnc\\results\\whole_brain_stats\\v4'\n",
    "ps['save_str'] = 'dff_12_60_60_long_bl_with_ep'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in excel file specifying location of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_fcn(str):\n",
    "    return str.replace(\"'\", \"\")\n",
    "converters = {0:c_fcn, 1:c_fcn}\n",
    "\n",
    "data_locs = pd.read_excel(ps['data_loc_file'], header=1, usecols=[1, 2], converters=converters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in transition information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = read_raw_transitions_from_excel(ps['trans_file'], adjust_frame_index=True)\n",
    "trans = recode_beh(trans, 'beh_before')\n",
    "trans = recode_beh(trans, 'beh_after')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function to calculate $\\Delta F/F$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dff(f, b, background=ps['background'], ep=ps['ep']):\n",
    "    return (f-b)/(b-background+ep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract dff along with behavior information from each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing dataset 1 of 65.\n",
      "Done processing dataset 2 of 65.\n",
      "Done processing dataset 3 of 65.\n",
      "Done processing dataset 4 of 65.\n",
      "Done processing dataset 5 of 65.\n",
      "Done processing dataset 6 of 65.\n",
      "Done processing dataset 7 of 65.\n",
      "Done processing dataset 8 of 65.\n",
      "Done processing dataset 9 of 65.\n",
      "Done processing dataset 10 of 65.\n",
      "Done processing dataset 11 of 65.\n",
      "Done processing dataset 12 of 65.\n",
      "Done processing dataset 13 of 65.\n",
      "Done processing dataset 14 of 65.\n",
      "Done processing dataset 15 of 65.\n",
      "Done processing dataset 16 of 65.\n",
      "Done processing dataset 17 of 65.\n",
      "Done processing dataset 18 of 65.\n",
      "Done processing dataset 19 of 65.\n",
      "Done processing dataset 20 of 65.\n",
      "Done processing dataset 22 of 65.\n",
      "Done processing dataset 23 of 65.\n",
      "Done processing dataset 24 of 65.\n",
      "Done processing dataset 25 of 65.\n",
      "Done processing dataset 27 of 65.\n",
      "Done processing dataset 29 of 65.\n",
      "Done processing dataset 30 of 65.\n",
      "Done processing dataset 31 of 65.\n",
      "Done processing dataset 32 of 65.\n",
      "Done processing dataset 36 of 65.\n",
      "Done processing dataset 40 of 65.\n",
      "Done processing dataset 41 of 65.\n",
      "Done processing dataset 42 of 65.\n",
      "Done processing dataset 43 of 65.\n",
      "Done processing dataset 44 of 65.\n",
      "Done processing dataset 45 of 65.\n",
      "Done processing dataset 46 of 65.\n",
      "Done processing dataset 47 of 65.\n",
      "Done processing dataset 48 of 65.\n",
      "Done processing dataset 49 of 65.\n",
      "Done processing dataset 50 of 65.\n",
      "Done processing dataset 51 of 65.\n",
      "Done processing dataset 52 of 65.\n",
      "Done processing dataset 53 of 65.\n",
      "Done processing dataset 54 of 65.\n",
      "Done processing dataset 55 of 65.\n",
      "Done processing dataset 56 of 65.\n",
      "Done processing dataset 57 of 65.\n",
      "Done processing dataset 58 of 65.\n",
      "Done processing dataset 59 of 65.\n",
      "Done processing dataset 60 of 65.\n",
      "Done processing dataset 61 of 65.\n",
      "Done processing dataset 62 of 65.\n",
      "Done processing dataset 63 of 65.\n",
      "Done processing dataset 64 of 65.\n",
      "Done processing dataset 65 of 65.\n"
     ]
    }
   ],
   "source": [
    "n_datasets = len(data_locs)\n",
    "event_annots = [None]*n_datasets\n",
    "\n",
    "unique_trans_subjs = trans['subject_id'].unique()\n",
    "n_trans_subjs = len(unique_trans_subjs)\n",
    "\n",
    "matched_trans_subjs = np.zeros(n_trans_subjs)\n",
    "matched_data_subjs = np.zeros(n_datasets)\n",
    "\n",
    "for d_i in range(n_datasets):\n",
    "    \n",
    "    # ===============================================================================\n",
    "    # Find the appropriate annotations for this experiment\n",
    "    data_main_folder = data_locs['Main folder'][d_i]\n",
    "    data_sub_folder = data_locs['Subfolder'][d_i]\n",
    "\n",
    "    # Parse the subject from the subfolder string\n",
    "    match_ind = match_annotation_subject_to_volume_subject(data_main_folder, data_sub_folder, unique_trans_subjs)\n",
    "\n",
    "    if match_ind is not None:\n",
    "\n",
    "        # If we can find this subject in the annotations, make sure we haven't matched to it before\n",
    "        if matched_trans_subjs[match_ind] == True:\n",
    "            raise(RuntimeError('Found a transition subject we already matched to: d_i=' + str(d_i)))\n",
    "        if matched_data_subjs[d_i] == True:\n",
    "            raise(RuntimeError('Found a volume subject we already matched to.'))\n",
    "            \n",
    "        matched_trans_subjs[match_ind] = True\n",
    "        matched_data_subjs[d_i] = True\n",
    "         \n",
    "        # ===============================================================================\n",
    "        # Now that we know we have a unique match, we do the rest of our processing\n",
    "\n",
    "        # Read in the data for this subject\n",
    "        dataset_path = (Path(ps['dataset_base_folder']) / data_main_folder / data_sub_folder / \n",
    "                        Path(ps['dataset_folder']) / '*.pkl')\n",
    "        dataset_file = glob.glob(str(dataset_path))[0]\n",
    "\n",
    "        # Load the dataset\n",
    "        with open(dataset_file, 'rb') as f:\n",
    "            dataset = ROIDataset.from_dict(pickle.load(f))\n",
    "        \n",
    "        # Calculate dff\n",
    "        f=dataset.ts_data[ps['f_ts_str']]['vls'][:]\n",
    "        b=dataset.ts_data[ps['bl_ts_str']]['vls'][:]\n",
    "        dff = calc_dff(f=f, b=b)\n",
    "        \n",
    "        # Extract dff for each event\n",
    "        sample_id = unique_trans_subjs[match_ind]\n",
    "        event_rows = trans['subject_id'] == sample_id\n",
    "        sample_events = copy.deepcopy(trans[event_rows])\n",
    "        n_sample_events = len(sample_events)\n",
    "        dff_before = [None]*n_sample_events\n",
    "        dff_during = [None]*n_sample_events\n",
    "        dff_after = [None]*n_sample_events\n",
    "        \n",
    "        for e_i in range(n_sample_events):\n",
    "            m_start = sample_events['Manipulation Start'].to_numpy()[e_i]\n",
    "            m_end = sample_events['Manipulation End'].to_numpy()[e_i]\n",
    "            \n",
    "            dff_before[e_i] = np.mean(dff[m_start-ps['n_smps']:m_start,:], axis=0)\n",
    "            dff_during[e_i] = np.mean(dff[m_start:m_end+1,:], axis=0)\n",
    "            dff_after[e_i] = np.mean(dff[m_end+1:m_end+ps['n_smps']+1,:], axis=0)\n",
    "            \n",
    "        sample_events['dff_before'] = dff_before\n",
    "        sample_events['dff_during'] = dff_during\n",
    "        sample_events['dff_after'] = dff_after\n",
    "            \n",
    "        # Save results\n",
    "        event_annots[d_i] = sample_events\n",
    "    \n",
    "        # Give user some feedback\n",
    "        print('Done processing dataset ' + str(d_i + 1) + ' of ' + str(n_datasets) + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_annots = pd.concat(event_annots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what subjects we have transitions for but no registerd volumes\n",
    "ignored_trans_subjs = unique_trans_subjs[np.logical_not(matched_trans_subjs)]\n",
    "\n",
    "# See what subjects we have registered volumes for but no transitions\n",
    "ignored_vol_subjs = data_locs[np.logical_not(matched_data_subjs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to: A:\\projects\\keller_vnc\\results\\whole_brain_stats\\v4\\dff_12_60_60_long_bl_with_ep_2020_10_18_15_22_35_534124.pkl\n"
     ]
    }
   ],
   "source": [
    "## Save results\n",
    "rs = dict()\n",
    "rs['ps'] = ps\n",
    "rs['event_annots'] = event_annots\n",
    "rs['ignored_trans_subjs'] = ignored_trans_subjs\n",
    "rs['ignored_vol_subjs'] = ignored_vol_subjs\n",
    "\n",
    "save_name = append_ts(ps['save_str']) + '.pkl'\n",
    "save_path = Path(ps['save_folder']) / save_name\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(rs, f)\n",
    "    \n",
    "print('Saved results to: ' + str(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
