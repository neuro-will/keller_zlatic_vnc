{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to extract and save $\\Delta F / F$ for whole making whole brain maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from janelia_core.dataprocessing.dataset import ROIDataset\n",
    "from janelia_core.utils.data_saving import append_ts\n",
    "\n",
    "from keller_zlatic_vnc.data_processing import extract_transitions\n",
    "from keller_zlatic_vnc.data_processing import match_annotation_subject_to_volume_subject\n",
    "from keller_zlatic_vnc.data_processing import read_raw_transitions_from_excel\n",
    "from keller_zlatic_vnc.data_processing import recode_beh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = dict()\n",
    "\n",
    "# Location of excel file specifying where the data for each experiment is saved relative to the base folder\n",
    "ps['data_loc_file'] = r'A:\\projects\\keller_vnc\\data\\experiment_data_locations.xlsx'\n",
    "\n",
    "# Location of excel file specifying transition information \n",
    "ps['trans_file'] = r'A:\\projects\\keller_vnc\\data\\extracted_dff_v2\\transition_list.xlsx'\n",
    "\n",
    "# Subfolder containing the dataset for each subject\n",
    "ps['dataset_folder'] = 'extracted'\n",
    "\n",
    "# Base folder where datasets are stored \n",
    "ps['dataset_base_folder'] =r'K:\\\\SV4'\n",
    "\n",
    "# Data to calculate Delta F/F for in each dataset\n",
    "ps['f_ts_str'] = 'f_1_5_5'\n",
    "ps['bl_ts_str'] = 'bl_1_5_5'\n",
    "\n",
    "ps['n_smps'] = 3 # Number of samples to take before and after a manipulation \n",
    "\n",
    "# Parameters for calculating dff\n",
    "ps['background'] = 100\n",
    "ps['ep'] = 20\n",
    "\n",
    "# Location to save results\n",
    "ps['save_folder'] = r'A:\\projects\\keller_vnc\\results\\whole_brain_stats'\n",
    "ps['save_str'] = 'dff_1_5_5_with_ep'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in excel file specifying location of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_fcn(str):\n",
    "    return str.replace(\"'\", \"\")\n",
    "converters = {0:c_fcn, 1:c_fcn}\n",
    "\n",
    "data_locs = pd.read_excel(ps['data_loc_file'], header=1, usecols=[1, 2], converters=converters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in transition information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = read_raw_transitions_from_excel(ps['trans_file'], adjust_frame_index=True)\n",
    "trans = recode_beh(trans, 'beh_before')\n",
    "trans = recode_beh(trans, 'beh_after')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function to calculate $\\Delta F/F$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dff(f, b, background=ps['background'], ep=ps['ep']):\n",
    "    return (f-b)/(b-background+ep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract dff along with behavior information from each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_datasets = len(data_locs)\n",
    "event_annots = [None]*n_datasets\n",
    "\n",
    "unique_trans_subjs = trans['sample_id'].unique()\n",
    "n_trans_subjs = len(unique_trans_subjs)\n",
    "\n",
    "matched_trans_subjs = np.zeros(n_trans_subjs)\n",
    "matched_data_subjs = np.zeros(n_datasets)\n",
    "\n",
    "for d_i in range(n_datasets):\n",
    "    \n",
    "    # ===============================================================================\n",
    "    # Find the appropriate annotations for this experiment\n",
    "    data_main_folder = data_locs['Main folder'][d_i]\n",
    "    data_sub_folder = data_locs['Subfolder'][d_i]\n",
    "\n",
    "    # Parse the subject from the subfolder string\n",
    "    match_ind = match_annotation_subject_to_volume_subject(data_main_folder, data_sub_folder, unique_trans_subjs)\n",
    "\n",
    "    if match_ind is not None:\n",
    "\n",
    "        # If we can find this subject in the annotations, make sure we haven't matched to it before\n",
    "        if matched_trans_subjs[match_ind] == True:\n",
    "            raise(RuntimeError('Found a transition subject we already matched to: d_i=' + str(d_i)))\n",
    "        if matched_data_subjs[d_i] == True:\n",
    "            raise(RuntimeError('Found a volume subject we already matched to.'))\n",
    "            \n",
    "        matched_trans_subjs[match_ind] = True\n",
    "        matched_data_subjs[d_i] = True\n",
    "         \n",
    "        # ===============================================================================\n",
    "        # Now that we know we have a unique match, we do the rest of our processing\n",
    "\n",
    "        # Read in the data for this subject\n",
    "        dataset_path = (Path(ps['dataset_base_folder']) / data_main_folder / data_sub_folder / \n",
    "                        Path(ps['dataset_folder']) / '*.pkl')\n",
    "        dataset_file = glob.glob(str(dataset_path))[0]\n",
    "\n",
    "        # Load the dataset\n",
    "        with open(dataset_file, 'rb') as f:\n",
    "            dataset = ROIDataset.from_dict(pickle.load(f))\n",
    "        \n",
    "        # Calculate dff\n",
    "        f=dataset.ts_data[ps['f_ts_str']]['vls'][:]\n",
    "        b=dataset.ts_data[ps['bl_ts_str']]['vls'][:]\n",
    "        dff = calc_dff(f=f, b=b)\n",
    "        \n",
    "        # Extract dff for each event\n",
    "        sample_id = unique_trans_subjs[match_ind]\n",
    "        event_rows = trans['sample_id'] == sample_id\n",
    "        sample_events = copy.deepcopy(trans[event_rows])\n",
    "        n_sample_events = len(sample_events)\n",
    "        dff_before = [None]*n_sample_events\n",
    "        dff_during = [None]*n_sample_events\n",
    "        dff_after = [None]*n_sample_events\n",
    "        \n",
    "        for e_i in range(n_sample_events):\n",
    "            m_start = sample_events['Manipulation Start'].to_numpy()[e_i]\n",
    "            m_end = sample_events['Manipulation End'].to_numpy()[e_i]\n",
    "            \n",
    "            dff_before[e_i] = np.mean(dff[m_start-ps['n_smps']:m_start,:], axis=0)\n",
    "            dff_during[e_i] = np.mean(dff[m_start:m_end+1,:], axis=0)\n",
    "            dff_after[e_i] = np.mean(dff[m_end+1:m_end+ps['n_smps']+1,:], axis=0)\n",
    "            \n",
    "        sample_events['dff_before'] = dff_before\n",
    "        sample_events['dff_during'] = dff_during\n",
    "        sample_events['dff_after'] = dff_after\n",
    "            \n",
    "        # Save results\n",
    "        event_annots[d_i] = sample_events\n",
    "    \n",
    "        # Give user some feedback\n",
    "        print('Done processing dataset ' + str(d_i + 1) + ' of ' + str(n_datasets) + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_annots = pd.concat(event_annots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what subjects we have transitions for but no registerd volumes\n",
    "ignored_trans_subjs = unique_trans_subjs[np.logical_not(matched_trans_subjs)]\n",
    "\n",
    "# See what subjects we have registered volumes for but no transitions\n",
    "ignored_vol_subjs = data_locs[np.logical_not(matched_data_subjs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save results\n",
    "rs = dict()\n",
    "rs['ps'] = ps\n",
    "rs['event_annots'] = event_annots\n",
    "rs['ignored_trans_subjs'] = ignored_trans_subjs\n",
    "rs['ignored_vol_subjs'] = ignored_vol_subjs\n",
    "\n",
    "save_name = append_ts(ps['save_str']) + '.pkl'\n",
    "save_path = Path(ps['save_folder']) / save_name\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(rs, f)\n",
    "    \n",
    "print('Saved results to: ' + str(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
