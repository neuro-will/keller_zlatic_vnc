{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests for state, decision or reporting dependence in DFF, referencing each behavior to another single condition (such as quiet) across all voxels in the brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from janelia_core.utils.data_saving import append_ts\n",
    "from janelia_core.stats.regression import grouped_linear_regression_ols_estimator\n",
    "from janelia_core.stats.regression import grouped_linear_regression_acm_stats\n",
    "\n",
    "from keller_zlatic_vnc.data_processing import count_unique_subjs_per_transition\n",
    "from keller_zlatic_vnc.data_processing import extract_transitions\n",
    "from keller_zlatic_vnc.linear_modeling import one_hot_from_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = {}\n",
    "\n",
    "# Specigy where the processed data (the produced of dff_extraction.ipynb) is stored\n",
    "ps['data_folder'] = r'A:\\projects\\keller_vnc\\results\\whole_brain_stats'\n",
    "ps['data_file'] = r'dff_1_5_5_with_ep_2020_08_27_12_32_48_526097.pkl'\n",
    "\n",
    "# Speccify a cut-off time\n",
    "ps['cut_off_time'] = 3.656\n",
    "\n",
    "# Specify manipulation target\n",
    "ps['manip_type'] = 'both' # 'both', 'A4' or 'A9'\n",
    "\n",
    "# Specify thresholds for number of subjects we need for each behavior\n",
    "ps['min_n_subjects_per_beh'] = 3\n",
    "\n",
    "# Specify the test type  Options are:\n",
    "#\n",
    "#   state_dependence - tests if dff after manipulation is sensitive to behavior before\n",
    "#   prediction_dependence - tests if dff before manipulation is sensitive to behavior after\n",
    "#   decision_dependence - tests if dff during manipulation is sensitive to behavior after\n",
    "#   before_reporting - tests if dff before manipulation is sensitive to behavior before\n",
    "#   after_reporting - tests if dff after manipulation is sensitive to behavior after\n",
    "#\n",
    "ps['test_type'] = 'decision_dependence'\n",
    "\n",
    "# Specify reference behavior\n",
    "ps['beh_ref'] = 'Q'\n",
    "\n",
    "# Alpha value for thresholding significance\n",
    "ps['alpha'] = .05\n",
    "\n",
    "# Specify where we save results\n",
    "ps['save_folder'] = r'\\\\dm11\\bishoplab\\projects\\keller_vnc\\results\\whole_brain_stats'\n",
    "ps['save_str'] = 'decision_dep_quiet_ref'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(ps['data_folder']) / Path(ps['data_file'])\n",
    "with open(data_path, 'rb') as f:\n",
    "    file_data = pickle.load(f)\n",
    "data = file_data['event_annots']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename a few columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns = {'Smp ID':'subject_id', 'Beh Before':'beh_before', 'Beh After':'beh_after'}, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply cut-off time to define succeeding quiet behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, data = extract_transitions(data, ps['cut_off_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Down select for manipulation target if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ps['manip_type'] == 'A4':\n",
    "    data = data[data['man_tgt'] == 'A4']\n",
    "elif ps['manip_type'] == 'A9':\n",
    "    data = data[data['man_tgt'] == 'A9']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove behaviors which are not present in enough subjects\n",
    "\n",
    "After removing these behaviors, we keep only events which start and stop with retained behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_subj_cnts = count_unique_subjs_per_transition(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (ps['test_type'] == 'state_dependence') or (ps['test_type'] == 'before_reporting'):\n",
    "    after_beh_th = 0\n",
    "    before_beh_th = ps['min_n_subjects_per_beh']\n",
    "elif ((ps['test_type'] == 'prediction_dependence') or (ps['test_type'] == 'after_reporting') or \n",
    "      (ps['test_type'] == 'decision_dependence')):\n",
    "    after_beh_th = ps['min_n_subjects_per_beh']\n",
    "    before_beh_th = 0\n",
    "else:\n",
    "    raise(ValueError('The test_type ' + ps['test_type'] + ' is not recognized.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_beh_sum = trans_subj_cnts.sum()\n",
    "after_behs = [b for b in after_beh_sum[after_beh_sum >= after_beh_th].index]\n",
    "\n",
    "before_beh_sum = trans_subj_cnts.sum(1)\n",
    "before_behs = [b for b in before_beh_sum[before_beh_sum >= before_beh_th].index]\n",
    "\n",
    "before_keep_rows = data['beh_before'].apply(lambda x: x in set(before_behs))\n",
    "after_keep_rows = data['beh_after'].apply(lambda x: x in set(after_behs))\n",
    "data = data[before_keep_rows & after_keep_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update our list of before and after behaviors\n",
    "\n",
    "We do this since by removing rows, some of our control behaviors may no longer be present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_trans_sub_cnts = count_unique_subjs_per_transition(data)\n",
    "new_after_beh_sum = new_trans_sub_cnts.sum()\n",
    "after_behs = [b for b in new_after_beh_sum[new_after_beh_sum > 0].index]\n",
    "new_before_beh_sum = new_trans_sub_cnts.sum(1)\n",
    "before_behs = [b for b in new_before_beh_sum[new_before_beh_sum>0].index]\n",
    "print('Using the following before behaviors: ' + str(before_behs))\n",
    "print('Using the following after behaviors: ' + str(after_behs))\n",
    "print(['Number of rows remaining in data: ' + str(len(data))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull out $\\Delta F/F$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (ps['test_type'] == 'state_dependence') or (ps['test_type'] == 'after_reporting'):\n",
    "    dff = np.stack(data['dff_after'].to_numpy())\n",
    "    print('Extracting dff after the manipulation.')\n",
    "elif (ps['test_type'] == 'prediction_dependence') or (ps['test_type'] == 'before_reporting'):\n",
    "    dff = np.stack(data['dff_before'].to_numpy())\n",
    "    print('Extracting dff before the manipulation.')\n",
    "elif ps['test_type'] == 'decision_dependence':\n",
    "    dff = np.stack(data['dff_during'].to_numpy())\n",
    "    print('Extracting dff during the manipulation.')\n",
    "else:\n",
    "    raise(ValueError('The test_type ' + ps['test_type'] + ' is not recognized.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find grouping of data by subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids = data['subject_id'].unique()\n",
    "g = np.zeros(len(data))\n",
    "for u_i, u_id in enumerate(unique_ids):\n",
    "    g[data['subject_id'] == u_id] = u_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function for calculating stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_f(x_i, y_i, g_i, alpha_i):\n",
    "    beta, acm, n_grps = grouped_linear_regression_ols_estimator(x=x_i, y=y_i, g=g_i)\n",
    "    stats = grouped_linear_regression_acm_stats(beta=beta, acm=acm, n_grps=n_grps, alpha=alpha_i)\n",
    "    stats['beta'] = beta\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit models and calculate stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_behs_ref = list(set(before_behs).difference(ps['beh_ref']))\n",
    "after_behs_ref = list(set(after_behs).difference(ps['beh_ref']))\n",
    "before_behs_ref = sorted(before_behs_ref)\n",
    "after_behs_ref = sorted(after_behs_ref)\n",
    "\n",
    "n_before_behs = len(before_behs_ref)\n",
    "n_after_behs = len(after_behs_ref)\n",
    "\n",
    "one_hot_data_ref, one_hot_vars_ref = one_hot_from_table(data, beh_before=before_behs_ref, beh_after=after_behs_ref)\n",
    "one_hot_data_ref = np.concatenate([one_hot_data_ref, np.ones([one_hot_data_ref.shape[0], 1])], axis=1)\n",
    "one_hot_vars_ref = one_hot_vars_ref + ['ref']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rois = dff.shape[1]\n",
    "full_stats = [stats_f(x_i=one_hot_data_ref, y_i=dff[:, r_i], g_i=g, alpha_i=ps['alpha']) for r_i in range(n_rois)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (ps['test_type'] == 'state_dependence') or (ps['test_type'] == 'before_reporting'):\n",
    "        test_behs = before_behs_ref\n",
    "        pull_inds = range(0, n_before_behs)\n",
    "        #test_betas = beta[0:n_before_behs]\n",
    "       # test_c_ints = stats['c_ints'][:, 0:n_before_behs]\n",
    "        #test_sig = stats['non_zero'][0:n_before_behs]\n",
    "elif ((ps['test_type'] == 'prediction_dependence') or (ps['test_type'] == 'after_reporting') or\n",
    "      (ps['test_type'] == 'decision_dependence')):\n",
    "        test_behs = after_behs_ref\n",
    "        pull_inds = range(n_before_behs, n_before_behs+n_after_behs)\n",
    "       # test_betas = beta[n_before_behs:n_before_behs+n_after_behs]\n",
    "        #test_c_ints = stats['c_ints'][:, n_before_behs:n_before_behs+n_after_behs]\n",
    "       # test_sig = stats['non_zero'][n_before_behs:n_before_behs+n_after_behs]\n",
    "else:\n",
    "        raise(ValueError('The test_type ' + ps['test_type'] + ' is not recognized.'))\n",
    "        \n",
    "beh_stats = dict()\n",
    "for b, p_i in zip(test_behs, pull_inds):\n",
    "    beh_stats[b] = dict()\n",
    "    beh_stats[b]['p_values'] = [rs_dict['non_zero_p'][p_i] for rs_dict in full_stats]\n",
    "    beh_stats[b]['beta'] = [rs_dict['beta'][p_i] for rs_dict in full_stats]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = append_ts(ps['test_type'] + '_' + ps['save_str']) + '.pkl'\n",
    "save_path = Path(ps['save_folder']) / save_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = dict()\n",
    "rs['beh_stats'] = beh_stats\n",
    "rs['full_stats'] = full_stats\n",
    "rs['ps'] = ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(rs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saved results to: ' + str(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
